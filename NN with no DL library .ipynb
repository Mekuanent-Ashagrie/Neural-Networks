{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@@@@@@@@@@@@@@@ Based on deeplearning course material by Andrew Ng @@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "#A very simple implementation of Forward and Backward propagation with no deep learning library \n",
    "#In this code you can find implementations for \n",
    "#-------A sigle hidden layer Neural Network \n",
    "#-------Random Weight initialization \n",
    "#-------The idea of Vectorization to get rid of unnecessary for loops\n",
    "#-------Activation functions [Relu, Leaky_Relu, tanh and sigmoid]\n",
    "#-------Derivatives of activation functions [Relu, Leaky_Relu, tanh and sigmoid]\n",
    "#-------Forward propagation \n",
    "#-------Backward propagation with gradient descent \n",
    "#-------Cost function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the training sets [simple example]\n",
    "X = [[1,1,1,1],[0,0,0,0],[1,1,0,0],[0,1,0,1]] #let equal number of a's and b's be class 0 and class 1 otherwise \n",
    "Y = [1,1,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup the hidden layers \n",
    "a1 = np.zeros((4,4)) #activations at first hidden layer [four hidden units and four inputs(weights)]\n",
    "a2 = np.zeros((1,4)) #activations at the output layer [one hidden unit and four inputs(weights)]\n",
    "z1 = np.zeros((4,4)) #linear funtions at the first hidden layer [four hidden units and four inputs(weights)]\n",
    "z2 = np.zeros((1,4)) #linear funtions at the output layer [one hidden unit and four inputs(weights)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialze the parameters \n",
    "w1 = np.random.randn(4,4)*0.01\n",
    "b1 = np.zeros((4,1))\n",
    "w2 = np.random.randn(4,1)*0.01\n",
    "b2 = 0 #single bias on the output node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement activation functions sigmoid, relu, tanh and leakyrelu\n",
    "sigmoid = np.vectorize(lambda val: 1/(1 + math.exp(-val)))\n",
    "\n",
    "tanh = np.vectorize(lambda val: (math.exp(val) - math.exp(-val))/(math.exp(val) + math.exp(-val)))\n",
    "\n",
    "relu = np.vectorize(lambda val: max(0, val))\n",
    "\n",
    "leaky_relu = np.vectorize(lambda val: max(0.0001*val, val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement derivations of each activation functions\n",
    "def der_sigmoid(z):\n",
    "    return (1/(1+math.exp(-z)))*(1-(1+math.exp(-z))) \n",
    "\n",
    "def der_tanh(z):    \n",
    "    return 1-(math.tanh(z)*math.tanh(z))\n",
    "\n",
    "def der_relu(z):\n",
    "    z[z>0]=1\n",
    "    z[z<0]=0\n",
    "    z[z==0]=0.00000000000001 \n",
    "    return z\n",
    "\n",
    "def der_leaky_relu(z): \n",
    "    z[z>0]=1\n",
    "    z[z<0]=0.01 #can even be smaller than this number \n",
    "    z[z==0]=0.00000000000001 \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.693147186754307\n",
      "Training Loss: 0.6931471866582404\n",
      "Training Loss: 0.6931471865633378\n",
      "Training Loss: 0.6931471864695873\n",
      "Training Loss: 0.693147186376977\n",
      "Training Loss: 0.6931471862854954\n",
      "Training Loss: 0.6931471861951313\n",
      "Training Loss: 0.6931471861058727\n",
      "Training Loss: 0.6931471860177087\n",
      "Training Loss: 0.6931471859306281\n",
      "Training Loss: 0.6931471858446197\n",
      "Training Loss: 0.6931471857596723\n",
      "Training Loss: 0.6931471856757758\n",
      "Training Loss: 0.6931471855929189\n",
      "Training Loss: 0.693147185511091\n",
      "Training Loss: 0.6931471854302819\n",
      "Training Loss: 0.6931471853504809\n",
      "Training Loss: 0.6931471852716777\n",
      "Training Loss: 0.6931471851938622\n",
      "Training Loss: 0.6931471851170243\n",
      "Training Loss: 0.6931471850411541\n",
      "Training Loss: 0.6931471849662414\n",
      "Training Loss: 0.6931471848922766\n",
      "Training Loss: 0.6931471848192499\n",
      "Training Loss: 0.6931471847471516\n",
      "Training Loss: 0.6931471846759725\n",
      "Training Loss: 0.693147184605703\n",
      "Training Loss: 0.6931471845363337\n",
      "Training Loss: 0.6931471844678558\n",
      "Training Loss: 0.6931471844002596\n",
      "Training Loss: 0.6931471843335362\n",
      "Training Loss: 0.6931471842676769\n",
      "Training Loss: 0.6931471842026724\n",
      "Training Loss: 0.6931471841385145\n",
      "Training Loss: 0.693147184075194\n",
      "Training Loss: 0.6931471840127026\n",
      "Training Loss: 0.6931471839510319\n",
      "Training Loss: 0.693147183890173\n",
      "Training Loss: 0.693147183830118\n",
      "Training Loss: 0.6931471837708585\n",
      "Training Loss: 0.6931471837123863\n",
      "Training Loss: 0.6931471836546932\n",
      "Training Loss: 0.6931471835977715\n",
      "Training Loss: 0.693147183541613\n",
      "Training Loss: 0.6931471834862101\n",
      "Training Loss: 0.6931471834315548\n",
      "Training Loss: 0.6931471833776395\n",
      "Training Loss: 0.6931471833244565\n",
      "Training Loss: 0.6931471832719984\n",
      "Training Loss: 0.6931471832202578\n",
      "Training Loss: 0.6931471831692273\n",
      "Training Loss: 0.6931471831189747\n",
      "Training Loss: 0.6931471830694016\n",
      "Training Loss: 0.6931471830205012\n",
      "Training Loss: 0.6931471829722673\n",
      "Training Loss: 0.6931471829246929\n",
      "Training Loss: 0.6931471828777718\n",
      "Training Loss: 0.6931471828314972\n",
      "Training Loss: 0.6931471827858632\n",
      "Training Loss: 0.6931471827408631\n",
      "Training Loss: 0.6931471826964911\n",
      "Training Loss: 0.6931471826527407\n",
      "Training Loss: 0.6931471826096058\n",
      "Training Loss: 0.6931471825670804\n",
      "Training Loss: 0.6931471825251585\n",
      "Training Loss: 0.6931471824838343\n",
      "Training Loss: 0.6931471824431019\n",
      "Training Loss: 0.6931471824029555\n",
      "Training Loss: 0.6931471823633892\n",
      "Training Loss: 0.6931471823243978\n",
      "Training Loss: 0.6931471822859749\n",
      "Training Loss: 0.6931471822481157\n",
      "Training Loss: 0.6931471822108141\n",
      "Training Loss: 0.6931471821740651\n",
      "Training Loss: 0.6931471821378632\n",
      "Training Loss: 0.6931471821022026\n",
      "Training Loss: 0.6931471820670787\n",
      "Training Loss: 0.6931471820324858\n",
      "Training Loss: 0.693147181998419\n",
      "Training Loss: 0.6931471819648731\n",
      "Training Loss: 0.6931471819318431\n",
      "Training Loss: 0.6931471818993237\n",
      "Training Loss: 0.6931471818673103\n",
      "Training Loss: 0.6931471818357977\n",
      "Training Loss: 0.6931471818047813\n",
      "Training Loss: 0.6931471817742562\n",
      "Training Loss: 0.6931471817442174\n",
      "Training Loss: 0.6931471817146604\n",
      "Training Loss: 0.6931471816855808\n",
      "Training Loss: 0.6931471816569734\n",
      "Training Loss: 0.6931471816288344\n",
      "Training Loss: 0.6931471816011586\n",
      "Training Loss: 0.6931471815739417\n",
      "Training Loss: 0.6931471815471797\n",
      "Training Loss: 0.6931471815208677\n",
      "Training Loss: 0.6931471814950017\n",
      "Training Loss: 0.6931471814695773\n",
      "Training Loss: 0.6931471814445902\n",
      "Training Loss: 0.6931471814200363\n",
      "Training Loss: 0.6931471813959116\n",
      "Training Loss: 0.6931471813722118\n",
      "Training Loss: 0.6931471813489327\n",
      "Training Loss: 0.6931471813260707\n",
      "Training Loss: 0.6931471813036215\n",
      "Training Loss: 0.6931471812815814\n",
      "Training Loss: 0.6931471812599463\n",
      "Training Loss: 0.6931471812387127\n",
      "Training Loss: 0.6931471812178762\n",
      "Training Loss: 0.6931471811974337\n",
      "Training Loss: 0.693147181177381\n",
      "Training Loss: 0.6931471811577149\n",
      "Training Loss: 0.6931471811384313\n",
      "Training Loss: 0.6931471811195269\n",
      "Training Loss: 0.6931471811009978\n",
      "Training Loss: 0.6931471810828408\n",
      "Training Loss: 0.6931471810650524\n",
      "Training Loss: 0.693147181047629\n",
      "Training Loss: 0.6931471810305673\n",
      "Training Loss: 0.6931471810138639\n",
      "Training Loss: 0.6931471809975155\n",
      "Training Loss: 0.6931471809815186\n",
      "Training Loss: 0.6931471809658705\n",
      "Training Loss: 0.6931471809505672\n",
      "Training Loss: 0.693147180935606\n",
      "Training Loss: 0.6931471809209839\n",
      "Training Loss: 0.6931471809066974\n",
      "Training Loss: 0.6931471808927434\n",
      "Training Loss: 0.6931471808791194\n",
      "Training Loss: 0.6931471808658215\n",
      "Training Loss: 0.6931471808528475\n",
      "Training Loss: 0.6931471808401943\n",
      "Training Loss: 0.6931471808278589\n",
      "Training Loss: 0.693147180815838\n",
      "Training Loss: 0.6931471808041295\n",
      "Training Loss: 0.6931471807927303\n",
      "Training Loss: 0.6931471807816373\n",
      "Training Loss: 0.6931471807708482\n",
      "Training Loss: 0.6931471807603601\n",
      "Training Loss: 0.6931471807501702\n",
      "Training Loss: 0.693147180740276\n",
      "Training Loss: 0.6931471807306748\n",
      "Training Loss: 0.6931471807213643\n",
      "Training Loss: 0.6931471807123414\n",
      "Training Loss: 0.6931471807036038\n",
      "Training Loss: 0.6931471806951492\n",
      "Training Loss: 0.6931471806869749\n",
      "Training Loss: 0.6931471806790785\n",
      "Training Loss: 0.6931471806714579\n",
      "Training Loss: 0.6931471806641101\n",
      "Training Loss: 0.693147180657033\n",
      "Training Loss: 0.6931471806502245\n",
      "Training Loss: 0.6931471806436822\n",
      "Training Loss: 0.6931471806374035\n",
      "Training Loss: 0.6931471806313866\n",
      "Training Loss: 0.6931471806256291\n",
      "Training Loss: 0.6931471806201288\n",
      "Training Loss: 0.6931471806148833\n",
      "Training Loss: 0.693147180609891\n",
      "Training Loss: 0.6931471806051491\n",
      "Training Loss: 0.6931471806006562\n",
      "Training Loss: 0.6931471805964098\n",
      "Training Loss: 0.6931471805924081\n",
      "Training Loss: 0.693147180588649\n",
      "Training Loss: 0.6931471805851303\n",
      "Training Loss: 0.6931471805818503\n",
      "Training Loss: 0.693147180578807\n",
      "Training Loss: 0.6931471805759986\n",
      "Training Loss: 0.6931471805734231\n",
      "Training Loss: 0.6931471805710783\n",
      "Training Loss: 0.693147180568963\n",
      "Training Loss: 0.693147180567075\n",
      "Training Loss: 0.6931471805654127\n",
      "Training Loss: 0.6931471805639741\n",
      "Training Loss: 0.6931471805627576\n",
      "Training Loss: 0.6931471805617613\n",
      "Training Loss: 0.6931471805609841\n",
      "Training Loss: 0.6931471805604235\n",
      "Training Loss: 0.6931471805600783\n",
      "Training Loss: 0.6931471805599468\n",
      "Training Loss: 0.6931471805600274\n",
      "Training Loss: 0.6931471805603185\n",
      "Training Loss: 0.6931471805608186\n",
      "Training Loss: 0.6931471805615259\n",
      "Training Loss: 0.6931471805624393\n",
      "Training Loss: 0.693147180563557\n",
      "Training Loss: 0.6931471805648775\n",
      "Training Loss: 0.6931471805663993\n",
      "Training Loss: 0.6931471805681213\n",
      "Training Loss: 0.6931471805700418\n",
      "Training Loss: 0.6931471805721595\n",
      "Training Loss: 0.6931471805744729\n",
      "Training Loss: 0.6931471805769808\n",
      "Training Loss: 0.6931471805796816\n",
      "Training Loss: 0.6931471805825744\n",
      "Training Loss: 0.6931471805856576\n",
      "Training Loss: 0.69314718058893\n",
      "Training Loss: 0.6931471805923903\n",
      "Training Loss: 0.6931471805960374\n",
      "Training Loss: 0.6931471805998699\n",
      "Training Loss: 0.6931471806038867\n",
      "Training Loss: 0.6931471806080867\n",
      "Training Loss: 0.6931471806124683\n",
      "Training Loss: 0.6931471806170308\n",
      "Training Loss: 0.693147180621773\n",
      "Training Loss: 0.6931471806266938\n",
      "Training Loss: 0.6931471806317919\n",
      "Training Loss: 0.6931471806370664\n",
      "Training Loss: 0.6931471806425162\n",
      "Training Loss: 0.6931471806481404\n",
      "Training Loss: 0.6931471806539377\n",
      "Training Loss: 0.6931471806599072\n",
      "Training Loss: 0.693147180666048\n",
      "Training Loss: 0.6931471806723591\n",
      "Training Loss: 0.6931471806788396\n",
      "Training Loss: 0.6931471806854885\n",
      "Training Loss: 0.6931471806923049\n",
      "Training Loss: 0.6931471806992877\n",
      "Training Loss: 0.6931471807064363\n",
      "Training Loss: 0.6931471807137498\n",
      "Training Loss: 0.6931471807212273\n",
      "Training Loss: 0.6931471807288677\n",
      "Training Loss: 0.6931471807366706\n",
      "Training Loss: 0.6931471807446349\n",
      "Training Loss: 0.69314718075276\n",
      "Training Loss: 0.693147180761045\n",
      "Training Loss: 0.6931471807694892\n",
      "Training Loss: 0.6931471807780919\n",
      "Training Loss: 0.6931471807868523\n",
      "Training Loss: 0.6931471807957698\n",
      "Training Loss: 0.6931471808048436\n",
      "Training Loss: 0.693147180814073\n",
      "Training Loss: 0.6931471808234573\n",
      "Training Loss: 0.693147180832996\n",
      "Training Loss: 0.6931471808426884\n",
      "Training Loss: 0.6931471808525338\n",
      "Training Loss: 0.6931471808625318\n",
      "Training Loss: 0.6931471808726817\n",
      "Training Loss: 0.6931471808829828\n",
      "Training Loss: 0.6931471808934347\n",
      "Training Loss: 0.6931471809040368\n",
      "Training Loss: 0.6931471809147886\n",
      "Training Loss: 0.6931471809256895\n",
      "Training Loss: 0.6931471809367391\n",
      "Training Loss: 0.6931471809479369\n",
      "Training Loss: 0.6931471809592824\n",
      "Training Loss: 0.6931471809707751\n",
      "Training Loss: 0.6931471809824145\n",
      "Training Loss: 0.6931471809942003\n",
      "Training Loss: 0.693147181006132\n",
      "Training Loss: 0.6931471810182093\n",
      "Training Loss: 0.6931471810304317\n",
      "Training Loss: 0.6931471810427989\n",
      "Training Loss: 0.6931471810553105\n",
      "Training Loss: 0.6931471810679661\n",
      "Training Loss: 0.6931471810807655\n",
      "Training Loss: 0.6931471810937081\n",
      "Training Loss: 0.6931471811067939\n",
      "Training Loss: 0.6931471811200223\n",
      "Training Loss: 0.6931471811333934\n",
      "Training Loss: 0.6931471811469065\n",
      "Training Loss: 0.6931471811605618\n",
      "Training Loss: 0.6931471811743587\n",
      "Training Loss: 0.6931471811882971\n",
      "Training Loss: 0.6931471812023768\n",
      "Training Loss: 0.6931471812165975\n",
      "Training Loss: 0.6931471812309591\n",
      "Training Loss: 0.6931471812454613\n",
      "Training Loss: 0.6931471812601042\n",
      "Training Loss: 0.6931471812748873\n",
      "Training Loss: 0.6931471812898107\n",
      "Training Loss: 0.693147181304874\n",
      "Training Loss: 0.6931471813200775\n",
      "Training Loss: 0.6931471813354206\n",
      "Training Loss: 0.6931471813509037\n",
      "Training Loss: 0.6931471813665264\n",
      "Training Loss: 0.6931471813822887\n",
      "Training Loss: 0.6931471813981906\n",
      "Training Loss: 0.693147181414232\n",
      "Training Loss: 0.6931471814304129\n",
      "Training Loss: 0.6931471814467333\n",
      "Training Loss: 0.693147181463193\n",
      "Training Loss: 0.6931471814797924\n",
      "Training Loss: 0.6931471814965311\n",
      "Training Loss: 0.6931471815134094\n",
      "Training Loss: 0.6931471815304273\n",
      "Training Loss: 0.6931471815475846\n",
      "Training Loss: 0.6931471815648818\n",
      "Training Loss: 0.6931471815823185\n",
      "Training Loss: 0.6931471815998952\n",
      "Training Loss: 0.6931471816176118\n",
      "Training Loss: 0.6931471816354684\n",
      "Training Loss: 0.6931471816534651\n",
      "Training Loss: 0.6931471816716024\n",
      "Training Loss: 0.6931471816898798\n",
      "Training Loss: 0.693147181708298\n",
      "Training Loss: 0.6931471817268571\n",
      "Training Loss: 0.6931471817455569\n",
      "Training Loss: 0.693147181764398\n",
      "Training Loss: 0.6931471817833805\n",
      "Training Loss: 0.6931471818025046\n",
      "Training Loss: 0.6931471818217705\n",
      "Training Loss: 0.6931471818411783\n",
      "Training Loss: 0.6931471818607287\n",
      "Training Loss: 0.6931471818804215\n",
      "Training Loss: 0.693147181900257\n",
      "Training Loss: 0.6931471819202359\n",
      "Training Loss: 0.6931471819403582\n",
      "Training Loss: 0.6931471819606242\n",
      "Training Loss: 0.6931471819810343\n",
      "Training Loss: 0.6931471820015888\n",
      "Training Loss: 0.6931471820222881\n",
      "Training Loss: 0.6931471820431323\n",
      "Training Loss: 0.6931471820641223\n",
      "Training Loss: 0.693147182085258\n",
      "Training Loss: 0.6931471821065399\n",
      "Training Loss: 0.6931471821279686\n",
      "Training Loss: 0.6931471821495443\n",
      "Training Loss: 0.6931471821712676\n",
      "Training Loss: 0.6931471821931386\n",
      "Training Loss: 0.6931471822151583\n",
      "Training Loss: 0.6931471822373266\n",
      "Training Loss: 0.6931471822596444\n",
      "Training Loss: 0.693147182282112\n",
      "Training Loss: 0.6931471823047299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6931471823274986\n",
      "Training Loss: 0.6931471823504186\n",
      "Training Loss: 0.6931471823734905\n",
      "Training Loss: 0.6931471823967149\n",
      "Training Loss: 0.6931471824200921\n",
      "Training Loss: 0.693147182443623\n",
      "Training Loss: 0.6931471824673079\n",
      "Training Loss: 0.6931471824911475\n",
      "Training Loss: 0.6931471825151425\n",
      "Training Loss: 0.6931471825392934\n",
      "Training Loss: 0.6931471825636009\n",
      "Training Loss: 0.6931471825880654\n",
      "Training Loss: 0.6931471826126878\n",
      "Training Loss: 0.6931471826374688\n",
      "Training Loss: 0.6931471826624089\n",
      "Training Loss: 0.6931471826875087\n",
      "Training Loss: 0.6931471827127693\n",
      "Training Loss: 0.693147182738191\n",
      "Training Loss: 0.6931471827637747\n",
      "Training Loss: 0.6931471827895211\n",
      "Training Loss: 0.693147182815431\n",
      "Training Loss: 0.6931471828415052\n",
      "Training Loss: 0.6931471828677441\n",
      "Training Loss: 0.693147182894149\n",
      "Training Loss: 0.6931471829207204\n",
      "Training Loss: 0.6931471829474591\n",
      "Training Loss: 0.6931471829743661\n",
      "Training Loss: 0.693147183001442\n",
      "Training Loss: 0.6931471830286876\n",
      "Training Loss: 0.6931471830561041\n",
      "Training Loss: 0.6931471830836922\n",
      "Training Loss: 0.6931471831114526\n",
      "Training Loss: 0.6931471831393863\n",
      "Training Loss: 0.6931471831674945\n",
      "Training Loss: 0.6931471831957776\n",
      "Training Loss: 0.6931471832242369\n",
      "Training Loss: 0.6931471832528732\n",
      "Training Loss: 0.6931471832816873\n",
      "Training Loss: 0.6931471833106806\n",
      "Training Loss: 0.6931471833398536\n",
      "Training Loss: 0.6931471833692078\n",
      "Training Loss: 0.6931471833987437\n",
      "Training Loss: 0.6931471834284624\n",
      "Training Loss: 0.6931471834583653\n",
      "Training Loss: 0.6931471834884532\n",
      "Training Loss: 0.6931471835187271\n",
      "Training Loss: 0.6931471835491881\n",
      "Training Loss: 0.6931471835798375\n",
      "Training Loss: 0.6931471836106761\n",
      "Training Loss: 0.693147183641705\n",
      "Training Loss: 0.6931471836729256\n",
      "Training Loss: 0.6931471837043389\n",
      "Training Loss: 0.693147183735946\n",
      "Training Loss: 0.693147183767748\n",
      "Training Loss: 0.6931471837997463\n",
      "Training Loss: 0.6931471838319418\n",
      "Training Loss: 0.6931471838643362\n",
      "Training Loss: 0.6931471838969299\n",
      "Training Loss: 0.693147183929725\n",
      "Training Loss: 0.6931471839627222\n",
      "Training Loss: 0.6931471839959229\n",
      "Training Loss: 0.6931471840293284\n",
      "Training Loss: 0.6931471840629401\n",
      "Training Loss: 0.693147184096759\n",
      "Training Loss: 0.6931471841307867\n",
      "Training Loss: 0.6931471841650245\n",
      "Training Loss: 0.6931471841994736\n",
      "Training Loss: 0.6931471842341355\n",
      "Training Loss: 0.6931471842690113\n",
      "Training Loss: 0.6931471843041026\n",
      "Training Loss: 0.693147184339411\n",
      "Training Loss: 0.6931471843749376\n",
      "Training Loss: 0.693147184410684\n",
      "Training Loss: 0.6931471844466515\n",
      "Training Loss: 0.6931471844828418\n",
      "Training Loss: 0.6931471845192562\n",
      "Training Loss: 0.6931471845558961\n",
      "Training Loss: 0.693147184592763\n",
      "Training Loss: 0.6931471846298589\n",
      "Training Loss: 0.6931471846671848\n",
      "Training Loss: 0.6931471847047426\n",
      "Training Loss: 0.6931471847425337\n",
      "Training Loss: 0.6931471847805596\n",
      "Training Loss: 0.693147184818822\n",
      "Training Loss: 0.6931471848573225\n",
      "Training Loss: 0.6931471848960629\n",
      "Training Loss: 0.6931471849350446\n",
      "Training Loss: 0.6931471849742694\n",
      "Training Loss: 0.693147185013739\n",
      "Training Loss: 0.693147185053455\n",
      "Training Loss: 0.6931471850934192\n",
      "Training Loss: 0.6931471851336333\n",
      "Training Loss: 0.693147185174099\n",
      "Training Loss: 0.6931471852148182\n",
      "Training Loss: 0.6931471852557926\n",
      "Training Loss: 0.693147185297024\n",
      "Training Loss: 0.693147185338514\n",
      "Training Loss: 0.6931471853802651\n",
      "Training Loss: 0.6931471854222784\n",
      "Training Loss: 0.6931471854645561\n",
      "Training Loss: 0.6931471855071003\n",
      "Training Loss: 0.6931471855499124\n",
      "Training Loss: 0.6931471855929947\n",
      "Training Loss: 0.693147185636349\n",
      "Training Loss: 0.6931471856799772\n",
      "Training Loss: 0.6931471857238816\n",
      "Training Loss: 0.6931471857680638\n",
      "Training Loss: 0.693147185812526\n",
      "Training Loss: 0.6931471858572703\n",
      "Training Loss: 0.6931471859022985\n",
      "Training Loss: 0.693147185947613\n",
      "Training Loss: 0.6931471859932157\n",
      "Training Loss: 0.6931471860391086\n",
      "Training Loss: 0.6931471860852941\n",
      "Training Loss: 0.6931471861317742\n",
      "Training Loss: 0.6931471861785511\n",
      "Training Loss: 0.693147186225627\n",
      "Training Loss: 0.6931471862730039\n",
      "Training Loss: 0.6931471863206842\n",
      "Training Loss: 0.6931471863686703\n",
      "Training Loss: 0.6931471864169644\n",
      "Training Loss: 0.6931471864655685\n",
      "Training Loss: 0.6931471865144851\n",
      "Training Loss: 0.6931471865637168\n",
      "Training Loss: 0.6931471866132655\n",
      "Training Loss: 0.6931471866631336\n",
      "Training Loss: 0.6931471867133241\n",
      "Training Loss: 0.6931471867638387\n",
      "Training Loss: 0.6931471868146799\n",
      "Training Loss: 0.6931471868658506\n",
      "Training Loss: 0.6931471869173532\n",
      "Training Loss: 0.6931471869691899\n",
      "Training Loss: 0.6931471870213634\n",
      "Training Loss: 0.6931471870738761\n",
      "Training Loss: 0.6931471871267307\n",
      "Training Loss: 0.69314718717993\n",
      "Training Loss: 0.6931471872334761\n",
      "Training Loss: 0.6931471872873721\n",
      "Training Loss: 0.6931471873416205\n",
      "Training Loss: 0.6931471873962238\n",
      "Training Loss: 0.6931471874511851\n",
      "Training Loss: 0.6931471875065067\n",
      "Training Loss: 0.6931471875621918\n",
      "Training Loss: 0.6931471876182428\n",
      "Training Loss: 0.6931471876746627\n",
      "Training Loss: 0.6931471877314543\n",
      "Training Loss: 0.6931471877886204\n",
      "Training Loss: 0.6931471878461638\n",
      "Training Loss: 0.6931471879040878\n",
      "Training Loss: 0.6931471879623948\n",
      "Training Loss: 0.693147188021088\n",
      "Training Loss: 0.6931471880801705\n",
      "Training Loss: 0.6931471881396452\n",
      "Training Loss: 0.693147188199515\n",
      "Training Loss: 0.693147188259783\n",
      "Training Loss: 0.6931471883204525\n",
      "Training Loss: 0.6931471883815266\n",
      "Training Loss: 0.693147188443008\n",
      "Training Loss: 0.6931471885049003\n",
      "Training Loss: 0.6931471885672067\n",
      "Training Loss: 0.6931471886299301\n",
      "Training Loss: 0.693147188693074\n",
      "Training Loss: 0.6931471887566417\n",
      "Training Loss: 0.6931471888206363\n",
      "Training Loss: 0.6931471888850614\n",
      "Training Loss: 0.6931471889499201\n",
      "Training Loss: 0.6931471890152158\n",
      "Training Loss: 0.6931471890809522\n",
      "Training Loss: 0.6931471891471326\n",
      "Training Loss: 0.6931471892137604\n",
      "Training Loss: 0.6931471892808391\n",
      "Training Loss: 0.6931471893483723\n",
      "Training Loss: 0.6931471894163637\n",
      "Training Loss: 0.6931471894848167\n",
      "Training Loss: 0.693147189553735\n",
      "Training Loss: 0.6931471896231223\n",
      "Training Loss: 0.6931471896929822\n",
      "Training Loss: 0.6931471897633185\n",
      "Training Loss: 0.693147189834135\n",
      "Training Loss: 0.6931471899054354\n",
      "Training Loss: 0.6931471899772236\n",
      "Training Loss: 0.6931471900495034\n",
      "Training Loss: 0.6931471901222787\n",
      "Training Loss: 0.6931471901955535\n",
      "Training Loss: 0.6931471902693316\n",
      "Training Loss: 0.693147190343617\n",
      "Training Loss: 0.693147190418414\n",
      "Training Loss: 0.6931471904937263\n",
      "Training Loss: 0.6931471905695582\n",
      "Training Loss: 0.6931471906459138\n",
      "Training Loss: 0.6931471907227973\n",
      "Training Loss: 0.6931471908002128\n",
      "Training Loss: 0.6931471908781646\n",
      "Training Loss: 0.6931471909566569\n",
      "Training Loss: 0.693147191035694\n",
      "Training Loss: 0.6931471911152802\n",
      "Training Loss: 0.6931471911954202\n",
      "Training Loss: 0.6931471912761181\n",
      "Training Loss: 0.6931471913573786\n",
      "Training Loss: 0.6931471914392058\n",
      "Training Loss: 0.6931471915216046\n",
      "Training Loss: 0.6931471916045795\n",
      "Training Loss: 0.6931471916881351\n",
      "Training Loss: 0.6931471917722759\n",
      "Training Loss: 0.6931471918570068\n",
      "Training Loss: 0.6931471919423324\n",
      "Training Loss: 0.6931471920282575\n",
      "Training Loss: 0.6931471921147869\n",
      "Training Loss: 0.6931471922019254\n",
      "Training Loss: 0.6931471922896781\n",
      "Training Loss: 0.6931471923780497\n",
      "Training Loss: 0.6931471924670454\n",
      "Training Loss: 0.6931471925566699\n",
      "Training Loss: 0.6931471926469287\n",
      "Training Loss: 0.6931471927378265\n",
      "Training Loss: 0.6931471928293687\n",
      "Training Loss: 0.6931471929215602\n",
      "Training Loss: 0.6931471930144067\n",
      "Training Loss: 0.6931471931079132\n",
      "Training Loss: 0.693147193202085\n",
      "Training Loss: 0.6931471932969275\n",
      "Training Loss: 0.6931471933924462\n",
      "Training Loss: 0.6931471934886466\n",
      "Training Loss: 0.693147193585534\n",
      "Training Loss: 0.6931471936831142\n",
      "Training Loss: 0.6931471937813926\n",
      "Training Loss: 0.6931471938803748\n",
      "Training Loss: 0.693147193980067\n",
      "Training Loss: 0.6931471940804744\n",
      "Training Loss: 0.6931471941816031\n",
      "Training Loss: 0.6931471942834588\n",
      "Training Loss: 0.6931471943860473\n",
      "Training Loss: 0.693147194489375\n",
      "Training Loss: 0.6931471945934475\n",
      "Training Loss: 0.6931471946982709\n",
      "Training Loss: 0.6931471948038515\n",
      "Training Loss: 0.6931471949101952\n",
      "Training Loss: 0.6931471950173084\n",
      "Training Loss: 0.6931471951251973\n",
      "Training Loss: 0.6931471952338681\n",
      "Training Loss: 0.6931471953433275\n",
      "Training Loss: 0.6931471954535816\n",
      "Training Loss: 0.693147195564637\n",
      "Training Loss: 0.6931471956765003\n",
      "Training Loss: 0.6931471957891779\n",
      "Training Loss: 0.6931471959026767\n",
      "Training Loss: 0.6931471960170033\n",
      "Training Loss: 0.6931471961321642\n",
      "Training Loss: 0.6931471962481667\n",
      "Training Loss: 0.6931471963650173\n",
      "Training Loss: 0.6931471964827232\n",
      "Training Loss: 0.6931471966012912\n",
      "Training Loss: 0.6931471967207283\n",
      "Training Loss: 0.6931471968410419\n",
      "Training Loss: 0.693147196962239\n",
      "Training Loss: 0.6931471970843269\n",
      "Training Loss: 0.6931471972073129\n",
      "Training Loss: 0.6931471973312042\n",
      "Training Loss: 0.6931471974560084\n",
      "Training Loss: 0.693147197581733\n",
      "Training Loss: 0.6931471977083856\n",
      "Training Loss: 0.6931471978359736\n",
      "Training Loss: 0.6931471979645049\n",
      "Training Loss: 0.6931471980939872\n",
      "Training Loss: 0.6931471982244282\n",
      "Training Loss: 0.6931471983558359\n",
      "Training Loss: 0.6931471984882184\n",
      "Training Loss: 0.6931471986215834\n",
      "Training Loss: 0.6931471987559392\n",
      "Training Loss: 0.6931471988912938\n",
      "Training Loss: 0.6931471990276558\n",
      "Training Loss: 0.6931471991650331\n",
      "Training Loss: 0.693147199303434\n",
      "Training Loss: 0.6931471994428675\n",
      "Training Loss: 0.6931471995833417\n",
      "Training Loss: 0.6931471997248653\n",
      "Training Loss: 0.693147199867447\n",
      "Training Loss: 0.6931472000110952\n",
      "Training Loss: 0.6931472001558194\n",
      "Training Loss: 0.6931472003016277\n",
      "Training Loss: 0.6931472004485297\n",
      "Training Loss: 0.6931472005965343\n",
      "Training Loss: 0.6931472007456505\n",
      "Training Loss: 0.6931472008958874\n",
      "Training Loss: 0.6931472010472546\n",
      "Training Loss: 0.6931472011997611\n",
      "Training Loss: 0.6931472013534166\n",
      "Training Loss: 0.6931472015082306\n",
      "Training Loss: 0.6931472016642126\n",
      "Training Loss: 0.6931472018213725\n",
      "Training Loss: 0.6931472019797198\n",
      "Training Loss: 0.6931472021392645\n",
      "Training Loss: 0.6931472023000166\n",
      "Training Loss: 0.693147202461986\n",
      "Training Loss: 0.6931472026251829\n",
      "Training Loss: 0.6931472027896175\n",
      "Training Loss: 0.6931472029553001\n",
      "Training Loss: 0.6931472031222411\n",
      "Training Loss: 0.6931472032904508\n",
      "Training Loss: 0.6931472034599401\n",
      "Training Loss: 0.6931472036307192\n",
      "Training Loss: 0.6931472038027993\n",
      "Training Loss: 0.693147203976191\n",
      "Training Loss: 0.6931472041509051\n",
      "Training Loss: 0.6931472043269529\n",
      "Training Loss: 0.6931472045043454\n",
      "Training Loss: 0.693147204683094\n",
      "Training Loss: 0.6931472048632098\n",
      "Training Loss: 0.6931472050447043\n",
      "Training Loss: 0.6931472052275888\n",
      "Training Loss: 0.6931472054118752\n",
      "Training Loss: 0.6931472055975753\n",
      "Training Loss: 0.6931472057847006\n",
      "Training Loss: 0.6931472059732632\n",
      "Training Loss: 0.6931472061632751\n",
      "Training Loss: 0.6931472063547484\n",
      "Training Loss: 0.6931472065476952\n",
      "Training Loss: 0.6931472067421282\n",
      "Training Loss: 0.6931472069380595\n",
      "Training Loss: 0.6931472071355018\n",
      "Training Loss: 0.6931472073344679\n",
      "Training Loss: 0.6931472075349705\n",
      "Training Loss: 0.6931472077370223\n",
      "Training Loss: 0.6931472079406364\n",
      "Training Loss: 0.6931472081458261\n",
      "Training Loss: 0.6931472083526045\n",
      "Training Loss: 0.6931472085609847\n",
      "Training Loss: 0.6931472087709805\n",
      "Training Loss: 0.6931472089826053\n",
      "Training Loss: 0.6931472091958728\n",
      "Training Loss: 0.6931472094107971\n",
      "Training Loss: 0.6931472096273916\n",
      "Training Loss: 0.6931472098456708\n",
      "Training Loss: 0.6931472100656488\n",
      "Training Loss: 0.6931472102873397\n",
      "Training Loss: 0.6931472105107582\n",
      "Training Loss: 0.6931472107359187\n",
      "Training Loss: 0.6931472109628359\n",
      "Training Loss: 0.6931472111915247\n",
      "Training Loss: 0.6931472114219999\n",
      "Training Loss: 0.6931472116542766\n",
      "Training Loss: 0.6931472118883699\n",
      "Training Loss: 0.6931472121242956\n",
      "Training Loss: 0.6931472123620687\n",
      "Training Loss: 0.6931472126017049\n",
      "Training Loss: 0.6931472128432201\n",
      "Training Loss: 0.6931472130866301\n",
      "Training Loss: 0.6931472133319507\n",
      "Training Loss: 0.6931472135791984\n",
      "Training Loss: 0.6931472138283894\n",
      "Training Loss: 0.69314721407954\n",
      "Training Loss: 0.6931472143326669\n",
      "Training Loss: 0.693147214587787\n",
      "Training Loss: 0.6931472148449169\n",
      "Training Loss: 0.6931472151040738\n",
      "Training Loss: 0.6931472153652748\n",
      "Training Loss: 0.6931472156285372\n",
      "Training Loss: 0.6931472158938787\n",
      "Training Loss: 0.6931472161613166\n",
      "Training Loss: 0.693147216430869\n",
      "Training Loss: 0.6931472167025537\n",
      "Training Loss: 0.6931472169763888\n",
      "Training Loss: 0.6931472172523928\n",
      "Training Loss: 0.6931472175305838\n",
      "Training Loss: 0.6931472178109805\n",
      "Training Loss: 0.6931472180936017\n",
      "Training Loss: 0.6931472183784663\n",
      "Training Loss: 0.6931472186655934\n",
      "Training Loss: 0.6931472189550022\n",
      "Training Loss: 0.6931472192467123\n",
      "Training Loss: 0.6931472195407429\n",
      "Training Loss: 0.6931472198371141\n",
      "Training Loss: 0.693147220135846\n",
      "Training Loss: 0.6931472204369583\n",
      "Training Loss: 0.6931472207404714\n",
      "Training Loss: 0.693147221046406\n",
      "Training Loss: 0.6931472213547826\n",
      "Training Loss: 0.693147221665622\n",
      "Training Loss: 0.6931472219789454\n",
      "Training Loss: 0.6931472222947738\n",
      "Training Loss: 0.6931472226131286\n",
      "Training Loss: 0.6931472229340316\n",
      "Training Loss: 0.6931472232575044\n",
      "Training Loss: 0.693147223583569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6931472239122476\n",
      "Training Loss: 0.6931472242435625\n",
      "Training Loss: 0.6931472245775363\n",
      "Training Loss: 0.6931472249141918\n",
      "Training Loss: 0.6931472252535518\n",
      "Training Loss: 0.6931472255956397\n",
      "Training Loss: 0.6931472259404787\n",
      "Training Loss: 0.6931472262880924\n",
      "Training Loss: 0.6931472266385047\n",
      "Training Loss: 0.6931472269917395\n",
      "Training Loss: 0.6931472273478209\n",
      "Training Loss: 0.6931472277067734\n",
      "Training Loss: 0.6931472280686217\n",
      "Training Loss: 0.6931472284333907\n",
      "Training Loss: 0.6931472288011054\n",
      "Training Loss: 0.6931472291717911\n",
      "Training Loss: 0.6931472295454733\n",
      "Training Loss: 0.6931472299221777\n",
      "Training Loss: 0.6931472303019305\n",
      "Training Loss: 0.6931472306847578\n",
      "Training Loss: 0.6931472310706861\n",
      "Training Loss: 0.6931472314597419\n",
      "Training Loss: 0.6931472318519523\n",
      "Training Loss: 0.6931472322473442\n",
      "Training Loss: 0.6931472326459454\n",
      "Training Loss: 0.6931472330477834\n",
      "Training Loss: 0.6931472334528859\n",
      "Training Loss: 0.6931472338612812\n",
      "Training Loss: 0.6931472342729976\n",
      "Training Loss: 0.6931472346880639\n",
      "Training Loss: 0.6931472351065089\n",
      "Training Loss: 0.6931472355283618\n",
      "Training Loss: 0.6931472359536518\n",
      "Training Loss: 0.6931472363824089\n",
      "Training Loss: 0.693147236814663\n",
      "Training Loss: 0.693147237250444\n",
      "Training Loss: 0.6931472376897827\n",
      "Training Loss: 0.6931472381327098\n",
      "Training Loss: 0.6931472385792563\n",
      "Training Loss: 0.6931472390294533\n",
      "Training Loss: 0.6931472394833327\n",
      "Training Loss: 0.6931472399409262\n",
      "Training Loss: 0.6931472404022659\n",
      "Training Loss: 0.6931472408673844\n",
      "Training Loss: 0.6931472413363144\n",
      "Training Loss: 0.6931472418090889\n",
      "Training Loss: 0.6931472422857411\n",
      "Training Loss: 0.6931472427663047\n",
      "Training Loss: 0.6931472432508137\n",
      "Training Loss: 0.6931472437393021\n",
      "Training Loss: 0.6931472442318047\n",
      "Training Loss: 0.6931472447283562\n",
      "Training Loss: 0.6931472452289917\n",
      "Training Loss: 0.6931472457337468\n",
      "Training Loss: 0.6931472462426572\n",
      "Training Loss: 0.693147246755759\n",
      "Training Loss: 0.6931472472730884\n",
      "Training Loss: 0.6931472477946827\n",
      "Training Loss: 0.6931472483205786\n",
      "Training Loss: 0.6931472488508136\n",
      "Training Loss: 0.6931472493854254\n",
      "Training Loss: 0.693147249924452\n",
      "Training Loss: 0.693147250467932\n",
      "Training Loss: 0.6931472510159042\n",
      "Training Loss: 0.6931472515684077\n",
      "Training Loss: 0.6931472521254819\n",
      "Training Loss: 0.6931472526871667\n",
      "Training Loss: 0.6931472532535022\n",
      "Training Loss: 0.693147253824529\n",
      "Training Loss: 0.6931472544002881\n",
      "Training Loss: 0.6931472549808207\n",
      "Training Loss: 0.6931472555661685\n",
      "Training Loss: 0.6931472561563735\n",
      "Training Loss: 0.693147256751478\n",
      "Training Loss: 0.6931472573515249\n",
      "Training Loss: 0.6931472579565576\n",
      "Training Loss: 0.6931472585666192\n",
      "Training Loss: 0.693147259181754\n",
      "Training Loss: 0.6931472598020063\n",
      "Training Loss: 0.6931472604274208\n",
      "Training Loss: 0.6931472610580427\n",
      "Training Loss: 0.6931472616939175\n",
      "Training Loss: 0.6931472623350912\n",
      "Training Loss: 0.6931472629816101\n",
      "Training Loss: 0.6931472636335214\n",
      "Training Loss: 0.6931472642908718\n",
      "Training Loss: 0.6931472649537092\n",
      "Training Loss: 0.6931472656220818\n",
      "Training Loss: 0.6931472662960381\n",
      "Training Loss: 0.6931472669756268\n",
      "Training Loss: 0.6931472676608976\n",
      "Training Loss: 0.6931472683519001\n",
      "Training Loss: 0.6931472690486848\n",
      "Training Loss: 0.6931472697513025\n",
      "Training Loss: 0.6931472704598041\n",
      "Training Loss: 0.6931472711742416\n",
      "Training Loss: 0.693147271894667\n",
      "Training Loss: 0.693147272621133\n",
      "Training Loss: 0.6931472733536926\n",
      "Training Loss: 0.6931472740923994\n",
      "Training Loss: 0.6931472748373075\n",
      "Training Loss: 0.6931472755884713\n",
      "Training Loss: 0.6931472763459461\n",
      "Training Loss: 0.6931472771097873\n",
      "Training Loss: 0.6931472778800508\n",
      "Training Loss: 0.6931472786567935\n",
      "Training Loss: 0.6931472794400722\n",
      "Training Loss: 0.6931472802299445\n",
      "Training Loss: 0.6931472810264686\n",
      "Training Loss: 0.693147281829703\n",
      "Training Loss: 0.693147282639707\n",
      "Training Loss: 0.6931472834565403\n",
      "Training Loss: 0.6931472842802631\n",
      "Training Loss: 0.6931472851109362\n",
      "Training Loss: 0.6931472859486207\n",
      "Training Loss: 0.693147286793379\n",
      "Training Loss: 0.6931472876452731\n",
      "Training Loss: 0.6931472885043664\n",
      "Training Loss: 0.6931472893707221\n",
      "Training Loss: 0.6931472902444047\n",
      "Training Loss: 0.6931472911254789\n",
      "Training Loss: 0.6931472920140098\n",
      "Training Loss: 0.6931472929100638\n",
      "Training Loss: 0.6931472938137071\n",
      "Training Loss: 0.6931472947250068\n",
      "Training Loss: 0.6931472956440311\n",
      "Training Loss: 0.693147296570848\n",
      "Training Loss: 0.6931472975055265\n",
      "Training Loss: 0.6931472984481364\n",
      "Training Loss: 0.6931472993987478\n",
      "Training Loss: 0.6931473003574318\n",
      "Training Loss: 0.6931473013242597\n",
      "Training Loss: 0.6931473022993039\n",
      "Training Loss: 0.6931473032826374\n",
      "Training Loss: 0.6931473042743332\n",
      "Training Loss: 0.6931473052744659\n",
      "Training Loss: 0.6931473062831104\n",
      "Training Loss: 0.6931473073003421\n",
      "Training Loss: 0.6931473083262374\n",
      "Training Loss: 0.693147309360873\n",
      "Training Loss: 0.6931473104043266\n",
      "Training Loss: 0.6931473114566767\n",
      "Training Loss: 0.6931473125180023\n",
      "Training Loss: 0.6931473135883831\n",
      "Training Loss: 0.6931473146678997\n",
      "Training Loss: 0.6931473157566335\n",
      "Training Loss: 0.693147316854666\n",
      "Training Loss: 0.6931473179620805\n",
      "Training Loss: 0.6931473190789603\n",
      "Training Loss: 0.6931473202053895\n",
      "Training Loss: 0.6931473213414533\n",
      "Training Loss: 0.6931473224872375\n",
      "Training Loss: 0.6931473236428287\n",
      "Training Loss: 0.6931473248083142\n",
      "Training Loss: 0.6931473259837821\n",
      "Training Loss: 0.6931473271693218\n",
      "Training Loss: 0.6931473283650227\n",
      "Training Loss: 0.6931473295709756\n",
      "Training Loss: 0.6931473307872719\n",
      "Training Loss: 0.693147332014004\n",
      "Training Loss: 0.6931473332512649\n",
      "Training Loss: 0.6931473344991487\n",
      "Training Loss: 0.6931473357577502\n",
      "Training Loss: 0.6931473370271652\n",
      "Training Loss: 0.6931473383074902\n",
      "Training Loss: 0.6931473395988229\n",
      "Training Loss: 0.6931473409012614\n",
      "Training Loss: 0.6931473422149051\n",
      "Training Loss: 0.6931473435398543\n",
      "Training Loss: 0.69314734487621\n",
      "Training Loss: 0.6931473462240743\n",
      "Training Loss: 0.6931473475835501\n",
      "Training Loss: 0.6931473489547415\n",
      "Training Loss: 0.6931473503377533\n",
      "Training Loss: 0.6931473517326913\n",
      "Training Loss: 0.6931473531396621\n",
      "Training Loss: 0.693147354558774\n",
      "Training Loss: 0.6931473559901353\n",
      "Training Loss: 0.6931473574338562\n",
      "Training Loss: 0.6931473588900472\n",
      "Training Loss: 0.6931473603588202\n",
      "Training Loss: 0.6931473618402881\n",
      "Training Loss: 0.6931473633345647\n",
      "Training Loss: 0.693147364841765\n",
      "Training Loss: 0.6931473663620049\n",
      "Training Loss: 0.6931473678954017\n",
      "Training Loss: 0.693147369442073\n",
      "Training Loss: 0.6931473710021385\n",
      "Training Loss: 0.6931473725757183\n",
      "Training Loss: 0.6931473741629339\n",
      "Training Loss: 0.693147375763908\n",
      "Training Loss: 0.693147377378764\n",
      "Training Loss: 0.6931473790076268\n",
      "Training Loss: 0.6931473806506223\n",
      "Training Loss: 0.6931473823078778\n",
      "Training Loss: 0.6931473839795212\n",
      "Training Loss: 0.6931473856656823\n",
      "Training Loss: 0.693147387366492\n",
      "Training Loss: 0.6931473890820816\n",
      "Training Loss: 0.6931473908125845\n",
      "Training Loss: 0.6931473925581348\n",
      "Training Loss: 0.6931473943188684\n",
      "Training Loss: 0.6931473960949219\n",
      "Training Loss: 0.6931473978864333\n",
      "Training Loss: 0.6931473996935422\n",
      "Training Loss: 0.6931474015163889\n",
      "Training Loss: 0.6931474033551155\n",
      "Training Loss: 0.6931474052098652\n",
      "Training Loss: 0.6931474070807826\n",
      "Training Loss: 0.6931474089680136\n",
      "Training Loss: 0.6931474108717053\n",
      "Training Loss: 0.6931474127920065\n",
      "Training Loss: 0.6931474147290672\n",
      "Training Loss: 0.6931474166830384\n",
      "Training Loss: 0.693147418654073\n",
      "Training Loss: 0.6931474206423256\n",
      "Training Loss: 0.6931474226479512\n",
      "Training Loss: 0.6931474246711072\n",
      "Training Loss: 0.693147426711952\n",
      "Training Loss: 0.6931474287706456\n",
      "Training Loss: 0.6931474308473493\n",
      "Training Loss: 0.6931474329422261\n",
      "Training Loss: 0.6931474350554405\n",
      "Training Loss: 0.6931474371871584\n",
      "Training Loss: 0.6931474393375473\n",
      "Training Loss: 0.6931474415067762\n",
      "Training Loss: 0.6931474436950159\n",
      "Training Loss: 0.6931474459024385\n",
      "Training Loss: 0.6931474481292176\n",
      "Training Loss: 0.6931474503755288\n",
      "Training Loss: 0.6931474526415491\n",
      "Training Loss: 0.6931474549274571\n",
      "Training Loss: 0.6931474572334332\n",
      "Training Loss: 0.6931474595596594\n",
      "Training Loss: 0.6931474619063193\n",
      "Training Loss: 0.6931474642735982\n",
      "Training Loss: 0.6931474666616835\n",
      "Training Loss: 0.6931474690707639\n",
      "Training Loss: 0.6931474715010297\n",
      "Training Loss: 0.6931474739526738\n",
      "Training Loss: 0.6931474764258899\n",
      "Training Loss: 0.6931474789208742\n",
      "Training Loss: 0.6931474814378243\n",
      "Training Loss: 0.69314748397694\n",
      "Training Loss: 0.6931474865384226\n",
      "Training Loss: 0.6931474891224756\n",
      "Training Loss: 0.693147491729304\n",
      "Training Loss: 0.693147494359115\n",
      "Training Loss: 0.6931474970121176\n",
      "Training Loss: 0.6931474996885231\n",
      "Training Loss: 0.693147502388544\n",
      "Training Loss: 0.6931475051123956\n",
      "Training Loss: 0.6931475078602946\n",
      "Training Loss: 0.6931475106324599\n",
      "Training Loss: 0.693147513429113\n",
      "Training Loss: 0.6931475162504761\n",
      "Training Loss: 0.6931475190967751\n",
      "Training Loss: 0.6931475219682366\n",
      "Training Loss: 0.6931475248650905\n",
      "Training Loss: 0.6931475277875677\n",
      "Training Loss: 0.6931475307359023\n",
      "Training Loss: 0.6931475337103298\n",
      "Training Loss: 0.6931475367110882\n",
      "Training Loss: 0.6931475397384177\n",
      "Training Loss: 0.6931475427925609\n",
      "Training Loss: 0.6931475458737623\n",
      "Training Loss: 0.6931475489822689\n",
      "Training Loss: 0.6931475521183302\n",
      "Training Loss: 0.6931475552821974\n",
      "Training Loss: 0.6931475584741249\n",
      "Training Loss: 0.6931475616943688\n",
      "Training Loss: 0.6931475649431875\n",
      "Training Loss: 0.6931475682208427\n",
      "Training Loss: 0.6931475715275974\n",
      "Training Loss: 0.6931475748637179\n",
      "Training Loss: 0.6931475782294728\n",
      "Training Loss: 0.6931475816251327\n",
      "Training Loss: 0.6931475850509715\n",
      "Training Loss: 0.6931475885072649\n",
      "Training Loss: 0.6931475919942919\n",
      "Training Loss: 0.6931475955123336\n",
      "Training Loss: 0.6931475990616739\n",
      "Training Loss: 0.6931476026425992\n",
      "Training Loss: 0.693147606255399\n",
      "Training Loss: 0.6931476099003647\n",
      "Training Loss: 0.6931476135777914\n",
      "Training Loss: 0.6931476172879764\n",
      "Training Loss: 0.6931476210312195\n",
      "Training Loss: 0.6931476248078242\n",
      "Training Loss: 0.6931476286180958\n",
      "Training Loss: 0.6931476324623433\n",
      "Training Loss: 0.6931476363408781\n",
      "Training Loss: 0.6931476402540145\n",
      "Training Loss: 0.6931476442020701\n",
      "Training Loss: 0.6931476481853652\n",
      "Training Loss: 0.6931476522042233\n",
      "Training Loss: 0.6931476562589705\n",
      "Training Loss: 0.6931476603499362\n",
      "Training Loss: 0.6931476644774532\n",
      "Training Loss: 0.693147668641857\n",
      "Training Loss: 0.6931476728434863\n",
      "Training Loss: 0.693147677082683\n",
      "Training Loss: 0.6931476813597923\n",
      "Training Loss: 0.6931476856751628\n",
      "Training Loss: 0.6931476900291457\n",
      "Training Loss: 0.6931476944220962\n",
      "Training Loss: 0.6931476988543724\n",
      "Training Loss: 0.6931477033263359\n",
      "Training Loss: 0.6931477078383519\n",
      "Training Loss: 0.6931477123907885\n",
      "Training Loss: 0.6931477169840178\n",
      "Training Loss: 0.6931477216184151\n",
      "Training Loss: 0.6931477262943591\n",
      "Training Loss: 0.6931477310122324\n",
      "Training Loss: 0.6931477357724207\n",
      "Training Loss: 0.6931477405753139\n",
      "Training Loss: 0.6931477454213053\n",
      "Training Loss: 0.6931477503107917\n",
      "Training Loss: 0.6931477552441737\n",
      "Training Loss: 0.693147760221856\n",
      "Training Loss: 0.6931477652442466\n",
      "Training Loss: 0.6931477703117576\n",
      "Training Loss: 0.6931477754248049\n",
      "Training Loss: 0.6931477805838084\n",
      "Training Loss: 0.6931477857891919\n",
      "Training Loss: 0.6931477910413831\n",
      "Training Loss: 0.6931477963408138\n",
      "Training Loss: 0.6931478016879196\n",
      "Training Loss: 0.6931478070831407\n",
      "Training Loss: 0.6931478125269209\n",
      "Training Loss: 0.6931478180197087\n",
      "Training Loss: 0.6931478235619563\n",
      "Training Loss: 0.6931478291541202\n",
      "Training Loss: 0.6931478347966618\n",
      "Training Loss: 0.693147840490046\n",
      "Training Loss: 0.6931478462347426\n",
      "Training Loss: 0.6931478520312258\n",
      "Training Loss: 0.6931478578799738\n",
      "Training Loss: 0.69314786378147\n",
      "Training Loss: 0.6931478697362017\n",
      "Training Loss: 0.6931478757446612\n",
      "Training Loss: 0.6931478818073452\n",
      "Training Loss: 0.6931478879247551\n",
      "Training Loss: 0.6931478940973974\n",
      "Training Loss: 0.6931479003257826\n",
      "Training Loss: 0.6931479066104267\n",
      "Training Loss: 0.6931479129518501\n",
      "Training Loss: 0.6931479193505784\n",
      "Training Loss: 0.6931479258071421\n",
      "Training Loss: 0.6931479323220765\n",
      "Training Loss: 0.6931479388959222\n",
      "Training Loss: 0.6931479455292245\n",
      "Training Loss: 0.6931479522225342\n",
      "Training Loss: 0.6931479589764075\n",
      "Training Loss: 0.693147965791405\n",
      "Training Loss: 0.6931479726680932\n",
      "Training Loss: 0.6931479796070441\n",
      "Training Loss: 0.6931479866088346\n",
      "Training Loss: 0.6931479936740472\n",
      "Training Loss: 0.6931480008032702\n",
      "Training Loss: 0.6931480079970969\n",
      "Training Loss: 0.6931480152561263\n",
      "Training Loss: 0.6931480225809639\n",
      "Training Loss: 0.6931480299722192\n",
      "Training Loss: 0.6931480374305096\n",
      "Training Loss: 0.6931480449564563\n",
      "Training Loss: 0.6931480525506877\n",
      "Training Loss: 0.6931480602138373\n",
      "Training Loss: 0.6931480679465454\n",
      "Training Loss: 0.6931480757494575\n",
      "Training Loss: 0.6931480836232256\n",
      "Training Loss: 0.6931480915685081\n",
      "Training Loss: 0.6931480995859689\n",
      "Training Loss: 0.693148107676279\n",
      "Training Loss: 0.6931481158401152\n",
      "Training Loss: 0.6931481240781605\n",
      "Training Loss: 0.693148132391105\n",
      "Training Loss: 0.6931481407796448\n",
      "Training Loss: 0.6931481492444829\n",
      "Training Loss: 0.6931481577863284\n",
      "Training Loss: 0.6931481664058978\n",
      "Training Loss: 0.693148175103914\n",
      "Training Loss: 0.6931481838811067\n",
      "Training Loss: 0.6931481927382125\n",
      "Training Loss: 0.6931482016759749\n",
      "Training Loss: 0.6931482106951448\n",
      "Training Loss: 0.6931482197964797\n",
      "Training Loss: 0.6931482289807445\n",
      "Training Loss: 0.6931482382487113\n",
      "Training Loss: 0.6931482476011595\n",
      "Training Loss: 0.6931482570388761\n",
      "Training Loss: 0.6931482665626548\n",
      "Training Loss: 0.6931482761732977\n",
      "Training Loss: 0.6931482858716139\n",
      "Training Loss: 0.6931482956584204\n",
      "Training Loss: 0.6931483055345418\n",
      "Training Loss: 0.6931483155008105\n",
      "Training Loss: 0.693148325558067\n",
      "Training Loss: 0.6931483357071593\n",
      "Training Loss: 0.6931483459489438\n",
      "Training Loss: 0.6931483562842848\n",
      "Training Loss: 0.6931483667140552\n",
      "Training Loss: 0.6931483772391354\n",
      "Training Loss: 0.6931483878604147\n",
      "Training Loss: 0.6931483985787906\n",
      "Training Loss: 0.6931484093951692\n",
      "Training Loss: 0.6931484203104651\n",
      "Training Loss: 0.6931484313256017\n",
      "Training Loss: 0.6931484424415106\n",
      "Training Loss: 0.6931484536591331\n",
      "Training Loss: 0.6931484649794188\n",
      "Training Loss: 0.6931484764033263\n",
      "Training Loss: 0.6931484879318236\n",
      "Training Loss: 0.6931484995658874\n",
      "Training Loss: 0.6931485113065041\n",
      "Training Loss: 0.6931485231546692\n",
      "Training Loss: 0.693148535111388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6931485471776744\n",
      "Training Loss: 0.6931485593545529\n",
      "Training Loss: 0.6931485716430572\n",
      "Training Loss: 0.6931485840442309\n",
      "Training Loss: 0.6931485965591275\n",
      "Training Loss: 0.6931486091888107\n",
      "Training Loss: 0.6931486219343536\n",
      "Training Loss: 0.6931486347968403\n",
      "Training Loss: 0.6931486477773646\n",
      "Training Loss: 0.693148660877031\n",
      "Training Loss: 0.6931486740969541\n",
      "Training Loss: 0.6931486874382595\n",
      "Training Loss: 0.6931487009020834\n",
      "Training Loss: 0.693148714489572\n",
      "Training Loss: 0.6931487282018838\n",
      "Training Loss: 0.693148742040187\n",
      "Training Loss: 0.6931487560056614\n",
      "Training Loss: 0.6931487700994982\n",
      "Training Loss: 0.6931487843228994\n",
      "Training Loss: 0.6931487986770786\n",
      "Training Loss: 0.6931488131632613\n",
      "Training Loss: 0.6931488277826839\n",
      "Training Loss: 0.693148842536595\n",
      "Training Loss: 0.6931488574262552\n",
      "Training Loss: 0.6931488724529367\n",
      "Training Loss: 0.6931488876179237\n",
      "Training Loss: 0.693148902922513\n",
      "Training Loss: 0.6931489183680133\n",
      "Training Loss: 0.693148933955746\n",
      "Training Loss: 0.693148949687045\n",
      "Training Loss: 0.6931489655632567\n",
      "Training Loss: 0.6931489815857403\n",
      "Training Loss: 0.6931489977558682\n",
      "Training Loss: 0.6931490140750257\n",
      "Training Loss: 0.6931490305446107\n",
      "Training Loss: 0.6931490471660352\n",
      "Training Loss: 0.6931490639407242\n",
      "Training Loss: 0.6931490808701162\n",
      "Training Loss: 0.6931490979556635\n",
      "Training Loss: 0.6931491151988322\n",
      "Training Loss: 0.6931491326011019\n",
      "Training Loss: 0.693149150163967\n",
      "Training Loss: 0.6931491678889355\n",
      "Training Loss: 0.6931491857775303\n",
      "Training Loss: 0.6931492038312876\n",
      "Training Loss: 0.6931492220517599\n",
      "Training Loss: 0.6931492404405128\n",
      "Training Loss: 0.6931492589991282\n",
      "Training Loss: 0.6931492777292017\n",
      "Training Loss: 0.6931492966323451\n",
      "Training Loss: 0.693149315710185\n",
      "Training Loss: 0.6931493349643636\n",
      "Training Loss: 0.6931493543965388\n",
      "Training Loss: 0.693149374008384\n",
      "Training Loss: 0.6931493938015885\n",
      "Training Loss: 0.6931494137778583\n",
      "Training Loss: 0.6931494339389148\n",
      "Training Loss: 0.6931494542864963\n",
      "Training Loss: 0.6931494748223571\n",
      "Training Loss: 0.6931494955482689\n",
      "Training Loss: 0.6931495164660195\n",
      "Training Loss: 0.6931495375774144\n",
      "Training Loss: 0.6931495588842757\n",
      "Training Loss: 0.6931495803884431\n",
      "Training Loss: 0.6931496020917738\n",
      "Training Loss: 0.6931496239961424\n",
      "Training Loss: 0.693149646103442\n",
      "Training Loss: 0.6931496684155829\n",
      "Training Loss: 0.693149690934494\n",
      "Training Loss: 0.6931497136621224\n",
      "Training Loss: 0.693149736600434\n",
      "Training Loss: 0.693149759751413\n",
      "Training Loss: 0.693149783117063\n",
      "Training Loss: 0.6931498066994058\n",
      "Training Loss: 0.6931498305004835\n",
      "Training Loss: 0.6931498545223569\n",
      "Training Loss: 0.6931498787671067\n",
      "Training Loss: 0.6931499032368331\n",
      "Training Loss: 0.6931499279336569\n",
      "Training Loss: 0.6931499528597186\n",
      "Training Loss: 0.6931499780171788\n",
      "Training Loss: 0.6931500034082195\n",
      "Training Loss: 0.6931500290350427\n",
      "Training Loss: 0.6931500548998719\n",
      "Training Loss: 0.6931500810049516\n",
      "Training Loss: 0.6931501073525472\n",
      "Training Loss: 0.6931501339449465\n",
      "Training Loss: 0.6931501607844586\n",
      "Training Loss: 0.6931501878734144\n",
      "Training Loss: 0.6931502152141673\n",
      "Training Loss: 0.6931502428090933\n",
      "Training Loss: 0.6931502706605907\n",
      "Training Loss: 0.6931502987710805\n",
      "Training Loss: 0.6931503271430071\n",
      "Training Loss: 0.6931503557788379\n",
      "Training Loss: 0.6931503846810642\n",
      "Training Loss: 0.6931504138522007\n",
      "Training Loss: 0.693150443294786\n",
      "Training Loss: 0.6931504730113832\n",
      "Training Loss: 0.6931505030045796\n",
      "Training Loss: 0.693150533276987\n",
      "Training Loss: 0.6931505638312422\n",
      "Training Loss: 0.6931505946700074\n",
      "Training Loss: 0.6931506257959699\n",
      "Training Loss: 0.6931506572118425\n",
      "Training Loss: 0.6931506889203641\n",
      "Training Loss: 0.6931507209242993\n",
      "Training Loss: 0.6931507532264398\n",
      "Training Loss: 0.693150785829603\n",
      "Training Loss: 0.6931508187366339\n",
      "Training Loss: 0.6931508519504038\n",
      "Training Loss: 0.6931508854738123\n",
      "Training Loss: 0.6931509193097859\n",
      "Training Loss: 0.6931509534612794\n",
      "Training Loss: 0.6931509879312753\n",
      "Training Loss: 0.6931510227227852\n",
      "Training Loss: 0.6931510578388489\n",
      "Training Loss: 0.6931510932825353\n",
      "Training Loss: 0.6931511290569425\n",
      "Training Loss: 0.6931511651651984\n",
      "Training Loss: 0.6931512016104603\n",
      "Training Loss: 0.6931512383959157\n",
      "Training Loss: 0.6931512755247828\n",
      "Training Loss: 0.6931513130003102\n",
      "Training Loss: 0.6931513508257775\n",
      "Training Loss: 0.6931513890044957\n",
      "Training Loss: 0.693151427539807\n",
      "Training Loss: 0.6931514664350859\n",
      "Training Loss: 0.6931515056937387\n",
      "Training Loss: 0.6931515453192046\n",
      "Training Loss: 0.6931515853149552\n",
      "Training Loss: 0.6931516256844955\n",
      "Training Loss: 0.6931516664313635\n",
      "Training Loss: 0.6931517075591314\n",
      "Training Loss: 0.6931517490714052\n",
      "Training Loss: 0.6931517909718252\n",
      "Training Loss: 0.6931518332640667\n",
      "Training Loss: 0.6931518759518399\n",
      "Training Loss: 0.6931519190388903\n",
      "Training Loss: 0.6931519625289991\n",
      "Training Loss: 0.6931520064259838\n",
      "Training Loss: 0.6931520507336981\n",
      "Training Loss: 0.6931520954560325\n",
      "Training Loss: 0.6931521405969144\n",
      "Training Loss: 0.6931521861603092\n",
      "Training Loss: 0.6931522321502193\n",
      "Training Loss: 0.6931522785706861\n",
      "Training Loss: 0.693152325425789\n",
      "Training Loss: 0.6931523727196464\n",
      "Training Loss: 0.6931524204564161\n",
      "Training Loss: 0.6931524686402956\n",
      "Training Loss: 0.6931525172755223\n",
      "Training Loss: 0.6931525663663738\n",
      "Training Loss: 0.693152615917169\n",
      "Training Loss: 0.6931526659322678\n",
      "Training Loss: 0.6931527164160711\n",
      "Training Loss: 0.6931527673730227\n",
      "Training Loss: 0.6931528188076084\n",
      "Training Loss: 0.6931528707243566\n",
      "Training Loss: 0.6931529231278387\n",
      "Training Loss: 0.6931529760226707\n",
      "Training Loss: 0.6931530294135114\n",
      "Training Loss: 0.6931530833050649\n",
      "Training Loss: 0.6931531377020796\n",
      "Training Loss: 0.6931531926093499\n",
      "Training Loss: 0.693153248031715\n",
      "Training Loss: 0.6931533039740607\n",
      "Training Loss: 0.6931533604413197\n",
      "Training Loss: 0.6931534174384715\n",
      "Training Loss: 0.6931534749705428\n",
      "Training Loss: 0.6931535330426087\n",
      "Training Loss: 0.6931535916597924\n",
      "Training Loss: 0.6931536508272661\n",
      "Training Loss: 0.6931537105502513\n",
      "Training Loss: 0.6931537708340194\n",
      "Training Loss: 0.6931538316838917\n",
      "Training Loss: 0.6931538931052409\n",
      "Training Loss: 0.6931539551034904\n",
      "Training Loss: 0.6931540176841156\n",
      "Training Loss: 0.6931540808526437\n",
      "Training Loss: 0.6931541446146552\n",
      "Training Loss: 0.6931542089757836\n",
      "Training Loss: 0.6931542739417159\n",
      "Training Loss: 0.6931543395181937\n",
      "Training Loss: 0.693154405711013\n",
      "Training Loss: 0.6931544725260256\n",
      "Training Loss: 0.6931545399691383\n",
      "Training Loss: 0.6931546080463151\n",
      "Training Loss: 0.6931546767635762\n",
      "Training Loss: 0.6931547461269996\n",
      "Training Loss: 0.6931548161427211\n",
      "Training Loss: 0.6931548868169352\n",
      "Training Loss: 0.693154958155895\n",
      "Training Loss: 0.693155030165914\n",
      "Training Loss: 0.6931551028533651\n",
      "Training Loss: 0.6931551762246827\n",
      "Training Loss: 0.6931552502863618\n",
      "Training Loss: 0.69315532504496\n",
      "Training Loss: 0.693155400507097\n",
      "Training Loss: 0.6931554766794559\n",
      "Training Loss: 0.6931555535687833\n",
      "Training Loss: 0.6931556311818904\n",
      "Training Loss: 0.6931557095256529\n",
      "Training Loss: 0.6931557886070128\n",
      "Training Loss: 0.693155868432977\n",
      "Training Loss: 0.6931559490106208\n",
      "Training Loss: 0.6931560303470858\n",
      "Training Loss: 0.6931561124495822\n",
      "Training Loss: 0.6931561953253886\n",
      "Training Loss: 0.6931562789818534\n",
      "Training Loss: 0.6931563634263948\n",
      "Training Loss: 0.6931564486665014\n",
      "Training Loss: 0.6931565347097342\n",
      "Training Loss: 0.6931566215637249\n",
      "Training Loss: 0.6931567092361791\n",
      "Training Loss: 0.6931567977348754\n",
      "Training Loss: 0.6931568870676662\n",
      "Training Loss: 0.6931569772424794\n",
      "Training Loss: 0.693157068267318\n",
      "Training Loss: 0.6931571601502615\n",
      "Training Loss: 0.6931572528994663\n",
      "Training Loss: 0.6931573465231663\n",
      "Training Loss: 0.6931574410296745\n",
      "Training Loss: 0.6931575364273825\n",
      "Training Loss: 0.6931576327247622\n",
      "Training Loss: 0.693157729930366\n",
      "Training Loss: 0.6931578280528279\n",
      "Training Loss: 0.6931579271008644\n",
      "Training Loss: 0.6931580270832747\n",
      "Training Loss: 0.6931581280089418\n",
      "Training Loss: 0.6931582298868335\n",
      "Training Loss: 0.6931583327260034\n",
      "Training Loss: 0.6931584365355906\n",
      "Training Loss: 0.6931585413248217\n",
      "Training Loss: 0.6931586471030111\n",
      "Training Loss: 0.6931587538795618\n",
      "Training Loss: 0.6931588616639669\n",
      "Training Loss: 0.693158970465809\n",
      "Training Loss: 0.6931590802947629\n",
      "Training Loss: 0.6931591911605949\n",
      "Training Loss: 0.6931593030731644\n",
      "Training Loss: 0.693159416042425\n",
      "Training Loss: 0.693159530078425\n",
      "Training Loss: 0.6931596451913079\n",
      "Training Loss: 0.6931597613913143\n",
      "Training Loss: 0.6931598786887823\n",
      "Training Loss: 0.6931599970941479\n",
      "Training Loss: 0.6931601166179472\n",
      "Training Loss: 0.693160237270816\n",
      "Training Loss: 0.6931603590634915\n",
      "Training Loss: 0.6931604820068131\n",
      "Training Loss: 0.6931606061117237\n",
      "Training Loss: 0.69316073138927\n",
      "Training Loss: 0.6931608578506037\n",
      "Training Loss: 0.6931609855069827\n",
      "Training Loss: 0.6931611143697727\n",
      "Training Loss: 0.6931612444504466\n",
      "Training Loss: 0.6931613757605869\n",
      "Training Loss: 0.6931615083118863\n",
      "Training Loss: 0.6931616421161487\n",
      "Training Loss: 0.6931617771852903\n",
      "Training Loss: 0.6931619135313408\n",
      "Training Loss: 0.6931620511664439\n",
      "Training Loss: 0.6931621901028592\n",
      "Training Loss: 0.6931623303529628\n",
      "Training Loss: 0.6931624719292484\n",
      "Training Loss: 0.6931626148443287\n",
      "Training Loss: 0.6931627591109363\n",
      "Training Loss: 0.6931629047419245\n",
      "Training Loss: 0.6931630517502695\n",
      "Training Loss: 0.6931632001490702\n",
      "Training Loss: 0.69316334995155\n",
      "Training Loss: 0.693163501171059\n",
      "Training Loss: 0.6931636538210729\n",
      "Training Loss: 0.6931638079151963\n",
      "Training Loss: 0.6931639634671629\n",
      "Training Loss: 0.6931641204908368\n",
      "Training Loss: 0.6931642790002139\n",
      "Training Loss: 0.6931644390094233\n",
      "Training Loss: 0.6931646005327279\n",
      "Training Loss: 0.6931647635845263\n",
      "Training Loss: 0.6931649281793542\n",
      "Training Loss: 0.6931650943318851\n",
      "Training Loss: 0.6931652620569316\n",
      "Training Loss: 0.6931654313694475\n",
      "Training Loss: 0.6931656022845283\n",
      "Training Loss: 0.693165774817413\n",
      "Training Loss: 0.6931659489834852\n",
      "Training Loss: 0.6931661247982743\n",
      "Training Loss: 0.6931663022774579\n",
      "Training Loss: 0.6931664814368617\n",
      "Training Loss: 0.6931666622924617\n",
      "Training Loss: 0.6931668448603858\n",
      "Training Loss: 0.6931670291569151\n",
      "Training Loss: 0.6931672151984847\n",
      "Training Loss: 0.6931674030016859\n",
      "Training Loss: 0.6931675925832674\n",
      "Training Loss: 0.6931677839601369\n",
      "Training Loss: 0.6931679771493624\n",
      "Training Loss: 0.693168172168174\n",
      "Training Loss: 0.6931683690339647\n",
      "Training Loss: 0.6931685677642928\n",
      "Training Loss: 0.6931687683768835\n",
      "Training Loss: 0.6931689708896291\n",
      "Training Loss: 0.6931691753205924\n",
      "Training Loss: 0.6931693816880071\n",
      "Training Loss: 0.6931695900102798\n",
      "Training Loss: 0.6931698003059913\n",
      "Training Loss: 0.693170012593899\n",
      "Training Loss: 0.6931702268929375\n",
      "Training Loss: 0.6931704432222214\n",
      "Training Loss: 0.693170661601046\n",
      "Training Loss: 0.6931708820488893\n",
      "Training Loss: 0.6931711045854143\n",
      "Training Loss: 0.6931713292304695\n",
      "Training Loss: 0.6931715560040923\n",
      "Training Loss: 0.6931717849265091\n",
      "Training Loss: 0.693172016018138\n",
      "Training Loss: 0.6931722492995905\n",
      "Training Loss: 0.6931724847916734\n",
      "Training Loss: 0.6931727225153901\n",
      "Training Loss: 0.6931729624919429\n",
      "Training Loss: 0.6931732047427346\n",
      "Training Loss: 0.6931734492893711\n",
      "Training Loss: 0.693173696153662\n",
      "Training Loss: 0.6931739453576234\n",
      "Training Loss: 0.6931741969234804\n",
      "Training Loss: 0.6931744508736668\n",
      "Training Loss: 0.6931747072308303\n",
      "Training Loss: 0.6931749660178312\n",
      "Training Loss: 0.6931752272577469\n",
      "Training Loss: 0.6931754909738728\n",
      "Training Loss: 0.6931757571897244\n",
      "Training Loss: 0.6931760259290388\n",
      "Training Loss: 0.6931762972157787\n",
      "Training Loss: 0.6931765710741323\n",
      "Training Loss: 0.6931768475285167\n",
      "Training Loss: 0.6931771266035791\n",
      "Training Loss: 0.6931774083242005\n",
      "Training Loss: 0.6931776927154959\n",
      "Training Loss: 0.693177979802818\n",
      "Training Loss: 0.6931782696117588\n",
      "Training Loss: 0.6931785621681517\n",
      "Training Loss: 0.6931788574980737\n",
      "Training Loss: 0.6931791556278489\n",
      "Training Loss: 0.6931794565840489\n",
      "Training Loss: 0.6931797603934962\n",
      "Training Loss: 0.6931800670832662\n",
      "Training Loss: 0.6931803766806904\n",
      "Training Loss: 0.6931806892133572\n",
      "Training Loss: 0.6931810047091157\n",
      "Training Loss: 0.6931813231960773\n",
      "Training Loss: 0.6931816447026188\n",
      "Training Loss: 0.6931819692573837\n",
      "Training Loss: 0.6931822968892865\n",
      "Training Loss: 0.6931826276275133\n",
      "Training Loss: 0.6931829615015255\n",
      "Training Loss: 0.6931832985410621\n",
      "Training Loss: 0.6931836387761423\n",
      "Training Loss: 0.6931839822370678\n",
      "Training Loss: 0.6931843289544253\n",
      "Training Loss: 0.6931846789590904\n",
      "Training Loss: 0.6931850322822286\n",
      "Training Loss: 0.6931853889552989\n",
      "Training Loss: 0.6931857490100563\n",
      "Training Loss: 0.6931861124785548\n",
      "Training Loss: 0.6931864793931495\n",
      "Training Loss: 0.6931868497865004\n",
      "Training Loss: 0.6931872236915743\n",
      "Training Loss: 0.693187601141648\n",
      "Training Loss: 0.6931879821703113\n",
      "Training Loss: 0.6931883668114693\n",
      "Training Loss: 0.6931887550993465\n",
      "Training Loss: 0.6931891470684883\n",
      "Training Loss: 0.693189542753765\n",
      "Training Loss: 0.6931899421903744\n",
      "Training Loss: 0.6931903454138449\n",
      "Training Loss: 0.6931907524600384\n",
      "Training Loss: 0.6931911633651536\n",
      "Training Loss: 0.6931915781657292\n",
      "Training Loss: 0.6931919968986461\n",
      "Training Loss: 0.6931924196011323\n",
      "Training Loss: 0.6931928463107645\n",
      "Training Loss: 0.6931932770654716\n",
      "Training Loss: 0.693193711903539\n",
      "Training Loss: 0.6931941508636104\n",
      "Training Loss: 0.6931945939846922\n",
      "Training Loss: 0.6931950413061564\n",
      "Training Loss: 0.6931954928677437\n",
      "Training Loss: 0.6931959487095677\n",
      "Training Loss: 0.6931964088721171\n",
      "Training Loss: 0.6931968733962606\n",
      "Training Loss: 0.693197342323249\n",
      "Training Loss: 0.6931978156947196\n",
      "Training Loss: 0.6931982935526997\n",
      "Training Loss: 0.6931987759396093\n",
      "Training Loss: 0.6931992628982662\n",
      "Training Loss: 0.6931997544718881\n",
      "Training Loss: 0.6932002507040973\n",
      "Training Loss: 0.6932007516389239\n",
      "Training Loss: 0.69320125732081\n",
      "Training Loss: 0.6932017677946127\n",
      "Training Loss: 0.6932022831056085\n",
      "Training Loss: 0.6932028032994973\n",
      "Training Loss: 0.6932033284224056\n",
      "Training Loss: 0.6932038585208908\n",
      "Training Loss: 0.6932043936419452\n",
      "Training Loss: 0.693204933833\n",
      "Training Loss: 0.6932054791419285\n",
      "Training Loss: 0.6932060296170517\n",
      "Training Loss: 0.6932065853071405\n",
      "Training Loss: 0.693207146261422\n",
      "Training Loss: 0.6932077125295811\n",
      "Training Loss: 0.6932082841617668\n",
      "Training Loss: 0.6932088612085956\n",
      "Training Loss: 0.6932094437211553\n",
      "Training Loss: 0.6932100317510104\n",
      "Training Loss: 0.6932106253502051\n",
      "Training Loss: 0.6932112245712689\n",
      "Training Loss: 0.6932118294672203\n",
      "Training Loss: 0.6932124400915713\n",
      "Training Loss: 0.6932130564983322\n",
      "Training Loss: 0.6932136787420154\n",
      "Training Loss: 0.6932143068776413\n",
      "Training Loss: 0.6932149409607413\n",
      "Training Loss: 0.6932155810473635\n",
      "Training Loss: 0.6932162271940772\n",
      "Training Loss: 0.693216879457977\n",
      "Training Loss: 0.6932175378966889\n",
      "Training Loss: 0.6932182025683735\n",
      "Training Loss: 0.6932188735317318\n",
      "Training Loss: 0.6932195508460102\n",
      "Training Loss: 0.6932202345710046\n",
      "Training Loss: 0.6932209247670662\n",
      "Training Loss: 0.6932216214951061\n",
      "Training Loss: 0.6932223248166006\n",
      "Training Loss: 0.6932230347935957\n",
      "Training Loss: 0.6932237514887132\n",
      "Training Loss: 0.6932244749651549\n",
      "Training Loss: 0.6932252052867088\n",
      "Training Loss: 0.6932259425177534\n",
      "Training Loss: 0.6932266867232636\n",
      "Training Loss: 0.6932274379688161\n",
      "Training Loss: 0.6932281963205946\n",
      "Training Loss: 0.6932289618453951\n",
      "Training Loss: 0.693229734610632\n",
      "Training Loss: 0.6932305146843429\n",
      "Training Loss: 0.6932313021351949\n",
      "Training Loss: 0.6932320970324897\n",
      "Training Loss: 0.6932328994461693\n",
      "Training Loss: 0.6932337094468226\n",
      "Training Loss: 0.6932345271056897\n",
      "Training Loss: 0.6932353524946693\n",
      "Training Loss: 0.6932361856863234\n",
      "Training Loss: 0.6932370267538838\n",
      "Training Loss: 0.6932378757712582\n",
      "Training Loss: 0.6932387328130358\n",
      "Training Loss: 0.6932395979544936\n",
      "Training Loss: 0.6932404712716028\n",
      "Training Loss: 0.6932413528410344\n",
      "Training Loss: 0.6932422427401659\n",
      "Training Loss: 0.693243141047088\n",
      "Training Loss: 0.6932440478406094\n",
      "Training Loss: 0.6932449632002653\n",
      "Training Loss: 0.6932458872063223\n",
      "Training Loss: 0.6932468199397853\n",
      "Training Loss: 0.6932477614824045\n",
      "Training Loss: 0.6932487119166814\n",
      "Training Loss: 0.693249671325876\n",
      "Training Loss: 0.6932506397940134\n",
      "Training Loss: 0.69325161740589\n",
      "Training Loss: 0.6932526042470815\n",
      "Training Loss: 0.6932536004039487\n",
      "Training Loss: 0.693254605963645\n",
      "Training Loss: 0.6932556210141232\n",
      "Training Loss: 0.693256645644143\n",
      "Training Loss: 0.6932576799432778\n",
      "Training Loss: 0.6932587240019215\n",
      "Training Loss: 0.6932597779112967\n",
      "Training Loss: 0.6932608417634609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6932619156513152\n",
      "Training Loss: 0.6932629996686105\n",
      "Training Loss: 0.6932640939099552\n",
      "Training Loss: 0.693265198470824\n",
      "Training Loss: 0.6932663134475636\n",
      "Training Loss: 0.6932674389374018\n",
      "Training Loss: 0.6932685750384546\n",
      "Training Loss: 0.6932697218497346\n",
      "Training Loss: 0.6932708794711582\n",
      "Training Loss: 0.6932720480035532\n",
      "Training Loss: 0.6932732275486685\n",
      "Training Loss: 0.6932744182091803\n",
      "Training Loss: 0.6932756200887016\n",
      "Training Loss: 0.6932768332917889\n",
      "Training Loss: 0.6932780579239523\n",
      "Training Loss: 0.6932792940916623\n",
      "Training Loss: 0.6932805419023591\n",
      "Training Loss: 0.6932818014644607\n",
      "Training Loss: 0.6932830728873713\n",
      "Training Loss: 0.6932843562814905\n",
      "Training Loss: 0.6932856517582211\n",
      "Training Loss: 0.6932869594299786\n",
      "Training Loss: 0.6932882794101998\n",
      "Training Loss: 0.6932896118133516\n",
      "Training Loss: 0.6932909567549398\n",
      "Training Loss: 0.6932923143515185\n",
      "Training Loss: 0.6932936847206987\n",
      "Training Loss: 0.6932950679811585\n",
      "Training Loss: 0.6932964642526508\n",
      "Training Loss: 0.6932978736560141\n",
      "Training Loss: 0.6932992963131808\n",
      "Training Loss: 0.6933007323471877\n",
      "Training Loss: 0.6933021818821841\n",
      "Training Loss: 0.6933036450434432\n",
      "Training Loss: 0.6933051219573703\n",
      "Training Loss: 0.6933066127515136\n",
      "Training Loss: 0.6933081175545734\n",
      "Training Loss: 0.6933096364964123\n",
      "Training Loss: 0.6933111697080652\n",
      "Training Loss: 0.6933127173217493\n",
      "Training Loss: 0.6933142794708743\n",
      "Training Loss: 0.6933158562900531\n",
      "Training Loss: 0.6933174479151112\n",
      "Training Loss: 0.6933190544830977\n",
      "Training Loss: 0.6933206761322961\n",
      "Training Loss: 0.6933223130022341\n",
      "Training Loss: 0.6933239652336947\n",
      "Training Loss: 0.6933256329687272\n",
      "Training Loss: 0.6933273163506574\n",
      "Training Loss: 0.693329015524099\n",
      "Training Loss: 0.6933307306349641\n",
      "Training Loss: 0.693332461830475\n",
      "Training Loss: 0.693334209259175\n",
      "Training Loss: 0.6933359730709392\n",
      "Training Loss: 0.6933377534169866\n",
      "Training Loss: 0.6933395504498909\n",
      "Training Loss: 0.6933413643235925\n",
      "Training Loss: 0.69334319519341\n",
      "Training Loss: 0.6933450432160515\n",
      "Training Loss: 0.6933469085496269\n",
      "Training Loss: 0.6933487913536591\n",
      "Training Loss: 0.6933506917890971\n",
      "Training Loss: 0.6933526100183269\n",
      "Training Loss: 0.6933545462051842\n",
      "Training Loss: 0.6933565005149664\n",
      "Training Loss: 0.6933584731144454\n",
      "Training Loss: 0.6933604641718792\n",
      "Training Loss: 0.6933624738570254\n",
      "Training Loss: 0.6933645023411532\n",
      "Training Loss: 0.6933665497970559\n",
      "Training Loss: 0.6933686163990644\n",
      "Training Loss: 0.69337070232306\n",
      "Training Loss: 0.693372807746486\n",
      "Training Loss: 0.6933749328483634\n",
      "Training Loss: 0.6933770778093014\n",
      "Training Loss: 0.6933792428115129\n",
      "Training Loss: 0.6933814280388261\n",
      "Training Loss: 0.6933836336766993\n",
      "Training Loss: 0.6933858599122339\n",
      "Training Loss: 0.6933881069341877\n",
      "Training Loss: 0.6933903749329902\n",
      "Training Loss: 0.6933926641007544\n",
      "Training Loss: 0.6933949746312924\n",
      "Training Loss: 0.693397306720129\n",
      "Training Loss: 0.6933996605645157\n",
      "Training Loss: 0.6934020363634452\n",
      "Training Loss: 0.693404434317666\n",
      "Training Loss: 0.6934068546296962\n",
      "Training Loss: 0.6934092975038394\n",
      "Training Loss: 0.693411763146198\n",
      "Training Loss: 0.6934142517646891\n",
      "Training Loss: 0.6934167635690589\n",
      "Training Loss: 0.6934192987708981\n",
      "Training Loss: 0.6934218575836566\n",
      "Training Loss: 0.6934244402226591\n",
      "Training Loss: 0.6934270469051208\n",
      "Training Loss: 0.6934296778501619\n",
      "Training Loss: 0.6934323332788246\n",
      "Training Loss: 0.6934350134140874\n",
      "Training Loss: 0.6934377184808819\n",
      "Training Loss: 0.6934404487061083\n",
      "Training Loss: 0.6934432043186517\n",
      "Training Loss: 0.6934459855493983\n",
      "Training Loss: 0.6934487926312509\n",
      "Training Loss: 0.6934516257991465\n",
      "Training Loss: 0.6934544852900724\n",
      "Training Loss: 0.693457371343082\n",
      "Training Loss: 0.6934602841993129\n",
      "Training Loss: 0.6934632241020027\n",
      "Training Loss: 0.6934661912965066\n",
      "Training Loss: 0.6934691860303139\n",
      "Training Loss: 0.6934722085530662\n",
      "Training Loss: 0.6934752591165735\n",
      "Training Loss: 0.6934783379748329\n",
      "Training Loss: 0.6934814453840452\n",
      "Training Loss: 0.6934845816026329\n",
      "Training Loss: 0.6934877468912584\n",
      "Training Loss: 0.6934909415128416\n",
      "Training Loss: 0.6934941657325779\n",
      "Training Loss: 0.6934974198179567\n",
      "Training Loss: 0.6935007040387793\n",
      "Training Loss: 0.6935040186671775\n",
      "Training Loss: 0.6935073639776326\n",
      "Training Loss: 0.6935107402469937\n",
      "Training Loss: 0.6935141477544962\n",
      "Training Loss: 0.6935175867817814\n",
      "Training Loss: 0.6935210576129152\n",
      "Training Loss: 0.693524560534407\n",
      "Training Loss: 0.6935280958352301\n",
      "Training Loss: 0.6935316638068398\n",
      "Training Loss: 0.693535264743194\n",
      "Training Loss: 0.6935388989407727\n",
      "Training Loss: 0.6935425666985969\n",
      "Training Loss: 0.6935462683182503\n",
      "Training Loss: 0.6935500041038978\n",
      "Training Loss: 0.693553774362307\n",
      "Training Loss: 0.6935575794028673\n",
      "Training Loss: 0.6935614195376116\n",
      "Training Loss: 0.6935652950812363\n",
      "Training Loss: 0.6935692063511221\n",
      "Training Loss: 0.6935731536673551\n",
      "Training Loss: 0.6935771373527474\n",
      "Training Loss: 0.6935811577328592\n",
      "Training Loss: 0.6935852151360191\n",
      "Training Loss: 0.6935893098933459\n",
      "Training Loss: 0.6935934423387704\n",
      "Training Loss: 0.6935976128090571\n",
      "Training Loss: 0.6936018216438256\n",
      "Training Loss: 0.6936060691855731\n",
      "Training Loss: 0.693610355779696\n",
      "Training Loss: 0.6936146817745132\n",
      "Training Loss: 0.6936190475212871\n",
      "Training Loss: 0.6936234533742472\n",
      "Training Loss: 0.6936278996906123\n",
      "Training Loss: 0.6936323868306138\n",
      "Training Loss: 0.6936369151575175\n",
      "Training Loss: 0.6936414850376486\n",
      "Training Loss: 0.6936460968404132\n",
      "Training Loss: 0.6936507509383224\n",
      "Training Loss: 0.6936554477070163\n",
      "Training Loss: 0.6936601875252868\n",
      "Training Loss: 0.6936649707751017\n",
      "Training Loss: 0.6936697978416291\n",
      "Training Loss: 0.6936746691132609\n",
      "Training Loss: 0.6936795849816373\n",
      "Training Loss: 0.6936845458416716\n",
      "Training Loss: 0.6936895520915738\n",
      "Training Loss: 0.6936946041328763\n",
      "Training Loss: 0.6936997023704579\n",
      "Training Loss: 0.6937048472125691\n",
      "Training Loss: 0.6937100390708577\n",
      "Training Loss: 0.6937152783603929\n",
      "Training Loss: 0.6937205654996919\n",
      "Training Loss: 0.6937259009107448\n",
      "Training Loss: 0.6937312850190404\n",
      "Training Loss: 0.6937367182535921\n",
      "Training Loss: 0.6937422010469639\n",
      "Training Loss: 0.6937477338352966\n",
      "Training Loss: 0.6937533170583341\n",
      "Training Loss: 0.6937589511594492\n",
      "Training Loss: 0.6937646365856712\n",
      "Training Loss: 0.6937703737877123\n",
      "Training Loss: 0.6937761632199939\n",
      "Training Loss: 0.6937820053406741\n",
      "Training Loss: 0.6937879006116753\n",
      "Training Loss: 0.6937938494987104\n",
      "Training Loss: 0.6937998524713113\n",
      "Training Loss: 0.693805910002856\n",
      "Training Loss: 0.6938120225705964\n",
      "Training Loss: 0.6938181906556868\n",
      "Training Loss: 0.6938244147432109\n",
      "Training Loss: 0.6938306953222109\n",
      "Training Loss: 0.6938370328857152\n",
      "Training Loss: 0.6938434279307676\n",
      "Training Loss: 0.6938498809584556\n",
      "Training Loss: 0.693856392473939\n",
      "Training Loss: 0.6938629629864793\n",
      "Training Loss: 0.6938695930094683\n",
      "Training Loss: 0.6938762830604579\n",
      "Training Loss: 0.693883033661189\n",
      "Training Loss: 0.6938898453376214\n",
      "Training Loss: 0.6938967186199633\n",
      "Training Loss: 0.6939036540427014\n",
      "Training Loss: 0.6939106521446303\n",
      "Training Loss: 0.6939177134688834\n",
      "Training Loss: 0.6939248385629628\n",
      "Training Loss: 0.6939320279787695\n",
      "Training Loss: 0.6939392822726348\n",
      "Training Loss: 0.6939466020053496\n",
      "Training Loss: 0.6939539877421972\n",
      "Training Loss: 0.6939614400529821\n",
      "Training Loss: 0.6939689595120635\n",
      "Training Loss: 0.6939765466983848\n",
      "Training Loss: 0.6939842021955058\n",
      "Training Loss: 0.6939919265916346\n",
      "Training Loss: 0.6939997204796581\n",
      "Training Loss: 0.6940075844571757\n",
      "Training Loss: 0.6940155191265301\n",
      "Training Loss: 0.6940235250948398\n",
      "Training Loss: 0.6940316029740312\n",
      "Training Loss: 0.6940397533808715\n",
      "Training Loss: 0.6940479769370014\n",
      "Training Loss: 0.6940562742689668\n",
      "Training Loss: 0.6940646460082532\n",
      "Training Loss: 0.6940730927913177\n",
      "Training Loss: 0.6940816152596223\n",
      "Training Loss: 0.6940902140596679\n",
      "Training Loss: 0.6940988898430269\n",
      "Training Loss: 0.6941076432663771\n",
      "Training Loss: 0.6941164749915355\n",
      "Training Loss: 0.6941253856854924\n",
      "Training Loss: 0.694134376020445\n",
      "Training Loss: 0.6941434466738318\n",
      "Training Loss: 0.6941525983283664\n",
      "Training Loss: 0.6941618316720729\n",
      "Training Loss: 0.6941711473983198\n",
      "Training Loss: 0.6941805462058539\n",
      "Training Loss: 0.6941900287988372\n",
      "Training Loss: 0.6941995958868796\n",
      "Training Loss: 0.6942092481850757\n",
      "Training Loss: 0.6942189864140387\n",
      "Training Loss: 0.6942288112999364\n",
      "Training Loss: 0.6942387235745272\n",
      "Training Loss: 0.6942487239751945\n",
      "Training Loss: 0.694258813244983\n",
      "Training Loss: 0.6942689921326346\n",
      "Training Loss: 0.6942792613926246\n",
      "Training Loss: 0.6942896217851972\n",
      "Training Loss: 0.6943000740764018\n",
      "Training Loss: 0.6943106190381301\n",
      "Training Loss: 0.6943212574481512\n",
      "Training Loss: 0.6943319900901499\n",
      "Training Loss: 0.6943428177537616\n",
      "Training Loss: 0.6943537412346104\n",
      "Training Loss: 0.6943647613343451\n",
      "Training Loss: 0.6943758788606772\n",
      "Training Loss: 0.6943870946274173\n",
      "Training Loss: 0.6943984094545123\n",
      "Training Loss: 0.6944098241680834\n",
      "Training Loss: 0.694421339600463\n",
      "Training Loss: 0.6944329565902327\n",
      "Training Loss: 0.6944446759822603\n",
      "Training Loss: 0.6944564986277385\n",
      "Training Loss: 0.6944684253842218\n",
      "Training Loss: 0.6944804571156658\n",
      "Training Loss: 0.6944925946924632\n",
      "Training Loss: 0.6945048389914846\n",
      "Training Loss: 0.6945171908961147\n",
      "Training Loss: 0.6945296512962917\n",
      "Training Loss: 0.694542221088545\n",
      "Training Loss: 0.6945549011760349\n",
      "Training Loss: 0.6945676924685898\n",
      "Training Loss: 0.6945805958827465\n",
      "Training Loss: 0.6945936123417875\n",
      "Training Loss: 0.6946067427757806\n",
      "Training Loss: 0.6946199881216185\n",
      "Training Loss: 0.6946333493230565\n",
      "Training Loss: 0.6946468273307528\n",
      "Training Loss: 0.6946604231023068\n",
      "Training Loss: 0.6946741376022996\n",
      "Training Loss: 0.6946879718023313\n",
      "Training Loss: 0.694701926681063\n",
      "Training Loss: 0.6947160032242543\n",
      "Training Loss: 0.6947302024248032\n",
      "Training Loss: 0.694744525282787\n",
      "Training Loss: 0.6947589728054998\n",
      "Training Loss: 0.6947735460074942\n",
      "Training Loss: 0.6947882459106203\n",
      "Training Loss: 0.6948030735440647\n",
      "Training Loss: 0.6948180299443921\n",
      "Training Loss: 0.6948331161555842\n",
      "Training Loss: 0.6948483332290794\n",
      "Training Loss: 0.6948636822238131\n",
      "Training Loss: 0.6948791642062584\n",
      "Training Loss: 0.6948947802504656\n",
      "Training Loss: 0.6949105314381019\n",
      "Training Loss: 0.6949264188584925\n",
      "Training Loss: 0.69494244360866\n",
      "Training Loss: 0.6949586067933654\n",
      "Training Loss: 0.6949749095251473\n",
      "Training Loss: 0.6949913529243632\n",
      "Training Loss: 0.6950079381192291\n",
      "Training Loss: 0.6950246662458602\n",
      "Training Loss: 0.6950415384483106\n",
      "Training Loss: 0.6950585558786144\n",
      "Training Loss: 0.6950757196968254\n",
      "Training Loss: 0.6950930310710578\n",
      "Training Loss: 0.6951104911775262\n",
      "Training Loss: 0.6951281012005863\n",
      "Training Loss: 0.6951458623327748\n",
      "Training Loss: 0.6951637757748497\n",
      "Training Loss: 0.6951818427358312\n",
      "Training Loss: 0.6952000644330412\n",
      "Training Loss: 0.6952184420921439\n",
      "Training Loss: 0.6952369769471858\n",
      "Training Loss: 0.6952556702406366\n",
      "Training Loss: 0.6952745232234283\n",
      "Training Loss: 0.6952935371549962\n",
      "Training Loss: 0.695312713303318\n",
      "Training Loss: 0.6953320529449554\n",
      "Training Loss: 0.6953515573650926\n",
      "Training Loss: 0.6953712278575768\n",
      "Training Loss: 0.6953910657249582\n",
      "Training Loss: 0.6954110722785302\n",
      "Training Loss: 0.695431248838368\n",
      "Training Loss: 0.6954515967333694\n",
      "Training Loss: 0.6954721173012943\n",
      "Training Loss: 0.6954928118888036\n",
      "Training Loss: 0.6955136818514995\n",
      "Training Loss: 0.6955347285539646\n",
      "Training Loss: 0.6955559533698008\n",
      "Training Loss: 0.6955773576816694\n",
      "Training Loss: 0.6955989428813294\n",
      "Training Loss: 0.6956207103696771\n",
      "Training Loss: 0.6956426615567849\n",
      "Training Loss: 0.6956647978619399\n",
      "Training Loss: 0.6956871207136832\n",
      "Training Loss: 0.6957096315498477\n",
      "Training Loss: 0.6957323318175976\n",
      "Training Loss: 0.6957552229734653\n",
      "Training Loss: 0.6957783064833913\n",
      "Training Loss: 0.6958015838227614\n",
      "Training Loss: 0.6958250564764447\n",
      "Training Loss: 0.6958487259388313\n",
      "Training Loss: 0.6958725937138707\n",
      "Training Loss: 0.6958966613151083\n",
      "Training Loss: 0.6959209302657238\n",
      "Training Loss: 0.6959454020985674\n",
      "Training Loss: 0.6959700783561977\n",
      "Training Loss: 0.6960004133996986\n",
      "Training Loss: 0.6960254998581268\n",
      "Training Loss: 0.6960507952363906\n",
      "Training Loss: 0.6960763011142221\n",
      "Training Loss: 0.6961020190812313\n",
      "Training Loss: 0.6961279507369402\n",
      "Training Loss: 0.6961540976908205\n",
      "Training Loss: 0.6961804615623276\n",
      "Training Loss: 0.6962129054318177\n",
      "Training Loss: 0.6962397039852515\n",
      "Training Loss: 0.696266724167136\n",
      "Training Loss: 0.6962939676350579\n",
      "Training Loss: 0.6963214360567482\n",
      "Training Loss: 0.6963491311101173\n",
      "Training Loss: 0.6963770544832883\n",
      "Training Loss: 0.6964052078746308\n",
      "Training Loss: 0.6964398905195612\n",
      "Training Loss: 0.696468504189963\n",
      "Training Loss: 0.696497352809772\n",
      "Training Loss: 0.6965264381163808\n",
      "Training Loss: 0.6965557618575791\n",
      "Training Loss: 0.6965853257915847\n",
      "Training Loss: 0.696615131687077\n",
      "Training Loss: 0.6966451813232271\n",
      "Training Loss: 0.696682239061703\n",
      "Training Loss: 0.6967127757059727\n",
      "Training Loss: 0.6967435612466976\n",
      "Training Loss: 0.6967745975028888\n",
      "Training Loss: 0.6968058863041885\n",
      "Training Loss: 0.6968374294908986\n",
      "Training Loss: 0.6968692289140108\n",
      "Training Loss: 0.6969084854001832\n",
      "Training Loss: 0.6969407960930243\n",
      "Training Loss: 0.6969733683766682\n",
      "Training Loss: 0.6970062041425856\n",
      "Training Loss: 0.6970393052930632\n",
      "Training Loss: 0.6970726737412314\n",
      "Training Loss: 0.6971063114110908\n",
      "Training Loss: 0.6971478805233114\n",
      "Training Loss: 0.6971820545966637\n",
      "Training Loss: 0.6972165034469786\n",
      "Training Loss: 0.6972512290393013\n",
      "Training Loss: 0.6972862333496612\n",
      "Training Loss: 0.6973215183650968\n",
      "Training Loss: 0.6973570860836789\n",
      "Training Loss: 0.697401086204457\n"
     ]
    }
   ],
   "source": [
    "iteration = 2000\n",
    "learning_rate = 0.01\n",
    "\n",
    "for i in range(0, iteration):\n",
    "    #make prediction with forward prop for all the observations \n",
    "    z1 = np.transpose(w1).dot(X) + b1 \n",
    "    a1 = relu(z1)\n",
    "    z2 = np.transpose(w2).dot(a1) + b2 \n",
    "    a2 = sigmoid(z2) #last layer with sigmoid activation function\n",
    "    \n",
    "    #now calculate the cost function j_wb \n",
    "    sum = 0;\n",
    "    for i in range(0, 4):\n",
    "        sum += (-(Y[i]*math.log(a2[0][1])+ (1-Y[i])*math.log(1-a2[0][1]) ))\n",
    "        j_wb = sum/4\n",
    "    print(\"Training Loss:\", str(j_wb))\n",
    "    \n",
    "    #Gradient descent with back propagation    \n",
    "    dz2 = a2 - Y #derivated through the chain rule of derivatives [derivative of sigmoid activation function * derivative of logistic cost function]\n",
    "    dw2 = (1/4)*np.dot(dz2,np.transpose(a1))\n",
    "    db2 = (1/4)*np.sum(dz2, axis=1, keepdims = True)\n",
    "    w2 = w2 - np.transpose(learning_rate*(dw2)) #update the weights \n",
    "    b2 = b2 - np.transpose(learning_rate*(db2)) #update the bias \n",
    "\n",
    "    dz1 = w2.dot(dz2)*der_relu(a1) #w2 must have been transpossed in this function but it is already transposed at the forward pass phase above \n",
    "    dw1 = (1/4)*(dz1.dot(np.transpose(X)))\n",
    "    db1 = (1/4)*(np.sum(dz1, axis=1, keepdims = True))\n",
    "    w1 = w1 - np.transpose(learning_rate*(dw1)) #update the weights \n",
    "    b1 = b1 - np.transpose(learning_rate*(db1)) #update the bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class:  0.0\n"
     ]
    }
   ],
   "source": [
    "#make a prediction[single forward pass with the last weights]\n",
    "X_test = [1,1,1,0]\n",
    "z1 = np.transpose(w1[0]).dot(X_test) + b1[0] \n",
    "a1 = relu(z1)\n",
    "z2 = np.transpose(w2).dot(a1) + b2 \n",
    "a2 = sigmoid(z2) \n",
    "print(\"Predicted class: \", str(round(a2[0][0], 0))) #bad result is expected as the model is trained only with 4 observations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Further extension\n",
    "#------Use big training data X\n",
    "#------Accept parameters from users input_shape, hidden layers, hidden units and others"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
